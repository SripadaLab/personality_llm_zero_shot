{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0f525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:42.692314Z",
     "start_time": "2025-06-02T01:20:42.679196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "def set_css_in_cell_output():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
    "            .widget-label {color: #d5d5d5 !important;}\n",
    "        </style>\n",
    "    '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css_in_cell_output)\n",
    "from IPython.core import ultratb\n",
    "ultratb.VerboseTB._tb_highlight = \"bg:#0D0D0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec3d23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:43.864924Z",
     "start_time": "2025-06-02T01:20:42.855724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tiktoken\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from jsonschema import validate\n",
    "from openai import OpenAI, RateLimitError, APITimeoutError, InternalServerError, Timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_incrementing, retry_if_exception_type, after_log, before_sleep_log\n",
    "import logging\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:44.180964Z",
     "start_time": "2025-06-02T01:20:43.867716Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = ''        # private openrouter api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c39d76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:44.186303Z",
     "start_time": "2025-06-02T01:20:44.182301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "request_limit_per_minute = 5000\n",
    "token_limit_per_minute = 2e6\n",
    "\n",
    "request_timeout_seconds = 120   # maximum wait time for openAI to respond before triggering request timeout \n",
    "request_max_retries = 1         # number to times to automatically retry failed requests\n",
    "tpm_wait_polling_seconds = 10    # if our internal TPM estimate thinks TPM limit is exceeded, how often to check if limit cleared\n",
    "\n",
    "# global logger for static classes\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbfee26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:44.236176Z",
     "start_time": "2025-06-02T01:20:44.211868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ChatGPT:\n",
    "    def __init__(self, model_provider_order,\n",
    "                 halt_on_error=True,\n",
    "                 is_verbose=True,\n",
    "                 timeout=request_timeout_seconds,\n",
    "                 max_retries=request_max_retries,\n",
    "                 request_limit_per_minute=request_limit_per_minute,\n",
    "                 token_limit_per_minute=token_limit_per_minute,\n",
    "                 tpm_wait_polling_seconds=tpm_wait_polling_seconds,\n",
    "                 logger=logger,\n",
    "                 api_key=OPENROUTER_API_KEY,\n",
    "                 limit_manager_db_password=LIMIT_MANAGER_DB_PASSWORD):\n",
    "        self.model, self.provider_order, self.model_canonical_name = model_provider_order\n",
    "        self.halt_on_error = halt_on_error\n",
    "        self.is_verbose = is_verbose\n",
    "        self.tpm_wait_polling_seconds = tpm_wait_polling_seconds\n",
    "        self.request_limit_per_minute = request_limit_per_minute\n",
    "        self.request_delay_seconds = 60.0 / request_limit_per_minute\n",
    "        self.token_limit_per_minute = token_limit_per_minute\n",
    "        self.response_history = []\n",
    "        self.message_history = {}\n",
    "        self.logger = logger\n",
    "        self.limit_manager_db_password = limit_manager_db_password\n",
    "        likert_options = [\n",
    "            \"Strongly Disagree\",\n",
    "            \"Strongly Agree\",\n",
    "            \"Disagree\",\n",
    "            \"Neutral\",\n",
    "            \"Agree\",\n",
    "        ]\n",
    "        # Sort by length to match longer options (e.g., \"Strongly Agree\") before shorter ones (e.g., \"Agree\")\n",
    "        self.likert_options = sorted(likert_options, key=len, reverse=True)\n",
    "        self.default_seed = 1 if 'x-ai' in self.model else 0\n",
    "        \n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\",\n",
    "                             api_key = api_key,\n",
    "                             timeout=timeout,\n",
    "                             max_retries=max_retries)\n",
    "\n",
    "        \n",
    "    def extract_likert_response(self, content):\n",
    "        content_lower = content.lower()\n",
    "        for option in self.likert_options:\n",
    "            pattern = r'\\b' + re.escape(option.lower()) + r'\\b'\n",
    "            match = re.search(pattern, content_lower)\n",
    "            if match:\n",
    "                return json.dumps({\"response\": option})\n",
    "        raise Exception(\"No Likert match found in: \", content)    \n",
    "    \n",
    "    \n",
    "    def get_running_cost_num_prompt_completion_tokens(self):\n",
    "        \"\"\"\n",
    "        This function computes the total cost (estimated) of all\n",
    "        messages sent by the instance of ChatGPT called from\n",
    "        Returns: total_running_cost, total_num_prompt_tokens, total_num_response_tokens\n",
    "        \"\"\"\n",
    "        n_prompt_tokens = np.sum([x.usage.prompt_tokens for x in self.response_history])\n",
    "        n_completion_tokens = np.sum([x.usage.completion_tokens for x in self.response_history])\n",
    "        total_cost = sum(r.usage.cost for r in self.response_history)\n",
    "        return (total_cost,\n",
    "                n_prompt_tokens,\n",
    "                n_completion_tokens)\n",
    "\n",
    "    def get_key_usage_credits(self):\n",
    "        # get what OpenRouter says the api key has used in total\n",
    "        # returns usage, total credits available\n",
    "        resp = requests.get(\n",
    "            \"https://openrouter.ai/api/v1/credits\",\n",
    "            headers={\"Authorization\": f\"Bearer {self.client.api_key}\"}\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        info = resp.json()[\"data\"]\n",
    "        return info[\"total_usage\"], info[\"total_credits\"]\n",
    "\n",
    "    # retry failing requests starting with 10 second wait,\n",
    "    # increasing wait time by 10 seconds each retry, up to a max window of 120s (or 5 times)\n",
    "    # the goal is to try to avoid hitting backoff,\n",
    "    # we treat this as a last resort because of its runtime cost\n",
    "    @retry(wait=wait_incrementing(start=10, increment=10, max=120),\n",
    "           stop=stop_after_attempt(5),\n",
    "           retry=retry_if_exception_type((RateLimitError, APITimeoutError, InternalServerError, Timeout)),\n",
    "           before_sleep=before_sleep_log(logger, logging.INFO),\n",
    "           after=after_log(logger, logging.INFO))\n",
    "    def completion_with_backoff(self, client, **kwargs):\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "    def check_internal_TPM_tracker(self, n_message_tokens):\n",
    "        \"\"\"\n",
    "        Checks internal TPM count to see if a message with length = n_message_tokens\n",
    "        can be sent. If not, it waits (sleeps - blocking) until the message delivery\n",
    "        meets into TPM limit\n",
    "        \"\"\"\n",
    "        now = datetime.datetime.now()\n",
    "        one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "        self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "        n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        # fixed delay waiting if TPM exceeded over past minute\n",
    "        # this is cpu polling, so it doesnt cost money or much compute\n",
    "        while n_tokens_past_minute > self.token_limit_per_minute:\n",
    "            if self.is_verbose: self.logger.info(f'Internal TPM limit exceeded, waiting for {self.tpm_wait_polling_seconds} seconds...')\n",
    "            time.sleep(self.tpm_wait_polling_seconds)\n",
    "            now = datetime.datetime.now()\n",
    "            one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "            self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "            n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        now = datetime.datetime.now()\n",
    "        self.token_count_history.append((n_message_tokens, now))\n",
    "\n",
    "\n",
    "    def send_message(self, system_role, message, validate_response=True):\n",
    "        \"\"\"\n",
    "        This is the primary function used to send messages to GPT and get responses\n",
    "        Steps are:\n",
    "          - check that json schema meets our basic requirements\n",
    "          - handle RPM and TPM limits as best as we can\n",
    "            (when openai rejects requests for exceeding limits its much slower)\n",
    "          - build and send the message using openai ChatCompletion api\n",
    "          - perform basic validation on GPT's response\n",
    "        This function either returns a ChatCompletion response object or None (if failure occurred)\n",
    "        Errors are propogated using raised Exceptions\n",
    "        \"\"\"\n",
    "        # sleep based on RPM limit (lazy logic, avoids keeping running count of actual requests per minute)\n",
    "        time.sleep(self.request_delay_seconds)\n",
    "\n",
    "        # check TPM limit (not lazy, uses running count of tokens per minute)\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "        n_message_tokens = len(encoding.encode(system_role)) + len(encoding.encode(message))\n",
    "        self.logger.info(f'processing message with {n_message_tokens} tokens...')\n",
    "        if n_message_tokens > self.token_limit_per_minute:\n",
    "            return self.bad_response_output(f'Unable to send message as it exceeds TPM. Number of tokens in message = {n_message_tokens}')\n",
    "        \n",
    "        # build and send message over openai-api\n",
    "        message_id = len(self.message_history.keys())\n",
    "        self.message_history[message_id] = [] if 'x-ai' in self.model else [{\"role\": \"system\", \"content\": system_role}]\n",
    "        self.message_history[message_id].append({\"role\": \"user\", \"content\": message})\n",
    "        try:\n",
    "            response = self.completion_with_backoff(self.client,\n",
    "                                                    model=self.model,\n",
    "                                                    messages=self.message_history[message_id],\n",
    "                                                    temperature=0,\n",
    "                                                    stream=False,\n",
    "                                                    extra_body={\"usage\": {\"include\": True},\n",
    "                                                                \"reasoning\": {# One of the following (not both):\n",
    "                                                                              \"effort\": \"medium\", # Can be \"high\", \"medium\", or \"low\" (OpenAI-style)\n",
    "                                                                              # Optional: Default is false. All models support this.\n",
    "                                                                              \"exclude\": False # Set to true to exclude reasoning tokens from response\n",
    "                                                                              },\n",
    "                                                                \"provider\": {\"order\": self.provider_order, \n",
    "                                                                             \"sort\": \"price\",\n",
    "                                                                             \"data_collection\": \"deny\",\n",
    "                                                                             \"allow_fallbacks\": False}},\n",
    "                                                    seed=self.default_seed, logprobs=False)\n",
    "            \n",
    "            self.response_history.append(response)\n",
    "            # reasoning models dont return a content field\n",
    "            if response.choices[0].message.content is None:\n",
    "                self.bad_response_output(f'None in message content')\n",
    "                return None\n",
    "            elif response.choices[0].message.content == '':\n",
    "                if hasattr(response.choices[0].message, 'reasoning'):\n",
    "                    if response.choices[0].message.reasoning != '':\n",
    "                        response.choices[0].message.content = response.choices[0].message.reasoning\n",
    "            else:\n",
    "                pass\n",
    "            try:\n",
    "                likert_response = self.extract_likert_response(response.choices[0].message.content.strip().lower())\n",
    "                response.choices[0].message.content = likert_response\n",
    "            except:\n",
    "                self.bad_response_output(f'GPT response didnt match likert options')\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if self.halt_on_error:\n",
    "                raise\n",
    "            else:\n",
    "                if self.is_verbose:\n",
    "                    str_e = str(e)\n",
    "                    self.logger.info(f'An exception occurred: {str_e}')\n",
    "                    self.logger.info(traceback.format_exc())\n",
    "                return None\n",
    "\n",
    "        return (response, message_id)\n",
    "        \n",
    "\n",
    "    def bad_response_output(self, error):\n",
    "        # general function for informing the user when an error occurs\n",
    "        if self.halt_on_error:\n",
    "            raise Exception(error)\n",
    "        else:\n",
    "            if self.is_verbose:\n",
    "                self.logger.info(f'Error - {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c0808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:44.242193Z",
     "start_time": "2025-06-02T01:20:44.237398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ac189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5aa97a4",
   "metadata": {},
   "source": [
    "# Example: We expect a simple string response from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_EVALUATE = gpt_41_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95702c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:44.678755Z",
     "start_time": "2025-06-02T01:20:44.671980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def example_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    response, message_history_id = None, -1\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message, \n",
    "                                                         validate_response=True)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        response_str = response_json[\"response\"]\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        response_str = ''\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    last_messages_sent_to_gpt = '' if (message_history_id not in chat.message_history) else chat.message_history[message_history_id]\n",
    "    print(f'Messages to GPT:\\n{last_messages_sent_to_gpt}')\n",
    "    print(f'Response from GPT:\\n{response_str}')\n",
    "    print(f'Cost: ${cost:.5f}')\n",
    "    \n",
    "    return response, message_history_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aeeb7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:47.007651Z",
     "start_time": "2025-06-02T01:20:44.817280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 271 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages to GPT:\n",
      "[{'role': 'system', 'content': 'you are a helpful assistant.'}, {'role': 'user', 'content': \"\\nSimulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\\n\\n---\\n\\nYour task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\\nBase your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\\n\\nFor each question, select the most appropriate option:\\n- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\\n- Disagree: The statement is mostly false or the participant would generally disagree with it.\\n- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\\n- Agree: The statement is mostly true or the participant would generally agree with it.\\n- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\\n\\nNEO-FFI question to answer:\\n{question}\\n\\nParticipant's spontaneous stream of thoughts:\\n{thoughts}\\n\\nYour response must be exactly one of:\\nStrongly Disagree  \\nDisagree  \\nNeutral  \\nAgree  \\nStrongly Agree\\n\\nDo not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\\n\"}]\n",
      "Response from GPT:\n",
      "Agree\n",
      "Cost: $0.00011\n"
     ]
    }
   ],
   "source": [
    "# specify system role and user message\n",
    "system_role = 'you are a helpful assistant.'\n",
    "likert_options = [\n",
    "    \"Strongly Disagree\",\n",
    "    \"Strongly Agree\",\n",
    "    \"Disagree\",\n",
    "    \"Neutral\",\n",
    "    \"Agree\",\n",
    "]\n",
    "likert_options_str = '\\n'.join(likert_options)\n",
    "\n",
    "message = \"\"\"\n",
    "Simulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\n",
    "\n",
    "---\n",
    "\n",
    "Your task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Disagree: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Agree: The statement is mostly true or the participant would generally agree with it.\n",
    "- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "NEO-FFI question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's spontaneous stream of thoughts:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Strongly Disagree  \n",
    "Disagree  \n",
    "Neutral  \n",
    "Agree  \n",
    "Strongly Agree\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "\n",
    "# create a single instance of ChatGPT \n",
    "# so that we can keep track of running costs\n",
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "response, message_history_id = example_messaging_wrapper(chat, system_role, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12edbb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:47.118165Z",
     "start_time": "2025-06-02T01:20:47.010168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1276.8750725725, 2250)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.get_key_usage_credits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3affef5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:47.123303Z",
     "start_time": "2025-06-02T01:20:47.119366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1748827245-QK3qVkYEIupflyyAc2pA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"response\": \"Agree\"}', refusal=None, role='assistant', function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1748827245, model='openai/gpt-4.1-mini', object='chat.completion', service_tier=None, system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=1, prompt_tokens=282, total_tokens=283, cost=0.0001144, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}), provider='OpenAI')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09d979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b01098",
   "metadata": {},
   "source": [
    "# SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec27c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:47.983298Z",
     "start_time": "2025-06-02T01:20:47.125188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509eb846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:47.993967Z",
     "start_time": "2025-06-02T01:20:47.984835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = './data/SST/'  # please contact the authors for access to the data\n",
    "\n",
    "good_th_dict = pickle.load(open(f'{DATA_ROOT}/SST_data.pickle', 'rb'))\n",
    "print(len([x for x in good_th_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c24367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:48.001495Z",
     "start_time": "2025-06-02T01:20:47.995316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    # list of 60 items from NEO-FFI\n",
    "    # redacted due to copyright\n",
    "    \"I am ...\",\n",
    "    \"I like ...\",\n",
    "    \"I don't ...\"\n",
    "]\n",
    "for i in range(len(questions)):\n",
    "    questions[i] = f'Question {i+1}: ' + questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab56af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:48.016885Z",
     "start_time": "2025-06-02T01:20:48.013057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_NEO_FFI_prompt(thoughts, question):\n",
    "    prompt_template = f\"\"\"\n",
    "Your task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Disagree: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Agree: The statement is mostly true or the participant would generally agree with it.\n",
    "- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "NEO-FFI question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's spontaneous stream of thoughts:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Strongly Disagree  \n",
    "Disagree  \n",
    "Neutral  \n",
    "Agree  \n",
    "Strongly Agree\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7db536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:48.596148Z",
     "start_time": "2025-06-02T01:20:48.590249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generic_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    message_history_id = -1\n",
    "    required_values = None\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        required_values = response_json['response']\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    return required_values, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3972fa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:49.141281Z",
     "start_time": "2025-06-02T01:20:49.137385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_role = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d61c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:52.422877Z",
     "start_time": "2025-06-02T01:20:52.414498Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def multiproc_neo_wrapper(thoughts, question_i, name, return_dict, model_to_evaluate=MODEL_TO_EVALUATE):\n",
    "    try:\n",
    "        chat = ChatGPT(model_provider_order=model_to_evaluate)\n",
    "        question = questions[question_i]\n",
    "        message = get_NEO_FFI_prompt(thoughts, question)\n",
    "        required_values, cost = generic_messaging_wrapper(chat, system_role, message)\n",
    "        return_dict[f'{name}'] = (required_values, cost)\n",
    "    except Exception as e:\n",
    "        pass  # silent failures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a2dcae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:20:53.572550Z",
     "start_time": "2025-06-02T01:20:53.567466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft limit: 4096 Hard limit: 262144\n",
      "New Soft limit: 30000 New Hard limit: 262144\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"Soft limit:\", soft, \"Hard limit:\", hard)\n",
    "# Raise soft limit (if you have permission):\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (30000, hard))\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"New Soft limit:\", soft, \"New Hard limit:\", hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84c54ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:20.118475Z",
     "start_time": "2025-06-02T01:24:20.104318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import glob\n",
    "import difflib\n",
    "\n",
    "\n",
    "def transcribe_and_merge_timestamps(sub_int):\n",
    "    assert sub_int in [int(x) for x in good_th_dict]\n",
    "    \n",
    "    matches = glob.glob(f'../../sst/whisper_outputs/*SST_{sub_int}.*.json')\n",
    "    if len(matches) == 0:\n",
    "        raise Exception(f'whisper output missing for subject: {sub_int}')\n",
    "    \n",
    "    assert len(matches) == 1\n",
    "    transcript_json = json.load(open(matches[0], 'rb'))\n",
    "    raw = [a for b in [transcript_json['segments'][i]['words'] for i in range(len(transcript_json['segments']))] for a in b]\n",
    "    clean = ' \\n'.join([a for b in good_th_dict[str(sub_int)] for a in b])\n",
    "\n",
    "    raw_tokens = [re.sub(r'[^\\w]', '', w['word']).lower() for w in raw]\n",
    "\n",
    "    # match either a newline or a word\n",
    "    tokens = re.findall(r'\\n|\\w+', clean)\n",
    "    # normalize words to lowercase, but leave '\\n' alone\n",
    "    clean_tokens = [\n",
    "        t if t == '\\n' else t.lower()\n",
    "        for t in tokens\n",
    "    ]\n",
    "\n",
    "    sm = SequenceMatcher(a=raw_tokens, b=clean_tokens)\n",
    "\n",
    "    # 4) Initialize aligned exactly matching cleaned tokens\n",
    "    aligned = [\n",
    "        {'word': clean_tokens[i], 'start': None, 'end': None}\n",
    "        for i in range(len(clean_tokens))\n",
    "    ]\n",
    "\n",
    "    # 5) Fill in timestamps for matching spans\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            for raw_i, clean_j in zip(range(i1, i2), range(j1, j2)):\n",
    "                aligned[clean_j]['start'] = raw[raw_i]['start']\n",
    "                aligned[clean_j]['end']   = raw[raw_i]['end']\n",
    "        # for 'replace', 'delete', 'insert': leave start/end as None\n",
    "\n",
    "    aligned[0]['start'] = 0\n",
    "    assert np.any([x['start'] is not None for x in aligned])\n",
    "\n",
    "\n",
    "    ### Data Assurance ###\n",
    "\n",
    "    # --- 1) Token-level equality check ---\n",
    "    aligned_words = [entry['word'] for entry in aligned]\n",
    "    assert aligned_words == clean_tokens, (\n",
    "        f\"Token sequences differ!\\n\"\n",
    "        f\"First mismatch at index {next(i for i,(a,b) in enumerate(zip(aligned_words, clean_tokens)) if a!=b)}:\\n\"\n",
    "        f\" aligned='{aligned_words[i]}', clean='{clean_tokens[i]}'\"\n",
    "    )\n",
    "\n",
    "    # 1) Recover the original lines (with their trailing spaces!)\n",
    "    original_lines = clean.split(' \\n')\n",
    "\n",
    "    # 2) Break your aligned tokens into the same number of lines\n",
    "    aligned_words = [e['word'] for e in aligned]\n",
    "    reconstructed_lines = []\n",
    "    cur = []\n",
    "    for tok in aligned_words:\n",
    "        if tok == '\\n':\n",
    "            reconstructed_lines.append(\" \".join(cur))\n",
    "            cur = []\n",
    "        else:\n",
    "            cur.append(tok)\n",
    "    # last line\n",
    "    reconstructed_lines.append(\" \".join(cur))\n",
    "\n",
    "    # 3) They must have the same length\n",
    "    assert len(original_lines) == len(reconstructed_lines), (\n",
    "        f\"Line-count mismatch: \"\n",
    "        f\"{len(original_lines)} original vs {len(reconstructed_lines)} reconstructed\"\n",
    "    )\n",
    "\n",
    "    # 4) Compare each line *exactly*\n",
    "    for idx, (orig, rec) in enumerate(zip(original_lines, reconstructed_lines)):\n",
    "        orig_cleaned_ws = re.sub(r\"\\s+\", \" \", orig.rstrip())\n",
    "        rec_cleaned_ws = re.sub(r\"\\s+\", \" \", rec.rstrip())\n",
    "        if orig_cleaned_ws != rec_cleaned_ws:\n",
    "            diff = \"\".join(difflib.ndiff([orig], [rec]))\n",
    "            raise AssertionError(f\"Line {idx} differs:\\n{diff!r}\")\n",
    "\n",
    "    # 5) Finally, stitch them back with the same separator and compare whole string\n",
    "    reconstructed = ' \\n'.join(reconstructed_lines)\n",
    "    \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cf2ef36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:20.601387Z",
     "start_time": "2025-06-02T01:24:20.595229Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_up_to_time(sub_int, time_limit):\n",
    "    aligned = transcribe_and_merge_timestamps(sub_int)\n",
    "    # 1) Collect tokens up to the time_limit\n",
    "    tokens = []\n",
    "    entries = []\n",
    "    for entry in aligned:\n",
    "        end = entry.get('end')\n",
    "        # stop when we hit a token whose start time exceeds the limit\n",
    "        if end is not None and end > time_limit:\n",
    "            break\n",
    "        tokens.append(entry['word'])\n",
    "        entries.append(entry)\n",
    "\n",
    "    # 1b) Remove any trailing newline tokens so you don't end up with an empty last line\n",
    "    while tokens and tokens[-1] == \"\\n\":\n",
    "        tokens.pop()\n",
    "        entries.pop()\n",
    "        \n",
    "    # 2) Regroup into lines at '\\n'\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    for tok in tokens:\n",
    "        if tok == \"\\n\":\n",
    "            lines.append(\" \".join(current_line))\n",
    "            current_line = []\n",
    "        else:\n",
    "            current_line.append(tok)\n",
    "    # append any trailing tokens as the last line\n",
    "    if current_line: lines.append(\" \".join(current_line))\n",
    "\n",
    "    # 3) Re-join lines with the same separator used to build `clean`\n",
    "    clean_reconstructed = \"\\n\".join(lines)  # use \" \\n\" here for matching \n",
    "    return clean_reconstructed, entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ac35bd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:32.456046Z",
     "start_time": "2025-06-02T01:24:29.113888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 30 minutes = 1800 seconds\n",
    "for sub in good_th_dict:\n",
    "    transcript, timestamps = clean_up_to_time(int(sub), 100)\n",
    "    #print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b8b374d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:33.441171Z",
     "start_time": "2025-06-02T01:24:33.435785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  60,  300,  600,  900, 1200, 1500, 1800])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MINUTE_CHUNKS = np.array([1, 5, 10, 15, 20, 25, 30]) * 60\n",
    "MINUTE_CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b1bec56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:34.503651Z",
     "start_time": "2025-06-02T01:24:34.486484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_until_resolved(good_th_dict, questions, MODEL_TO_EVALUATE,\n",
    "                       max_attempts=5, sleep_between=0.1, max_concurrent_calls=1000):\n",
    "    \n",
    "    return_dict = Manager().dict()\n",
    "    \n",
    "    n_subs = len(good_th_dict)\n",
    "    total_requests = len(good_th_dict) * len(questions) * len(MINUTE_CHUNKS)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def format_time(seconds):\n",
    "        mins, secs = divmod(int(seconds), 60)\n",
    "        hrs, mins = divmod(mins, 60)\n",
    "        return f\"{hrs:02d}:{mins:02d}:{secs:02d}\"\n",
    "    \n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        procs = []\n",
    "        n_missing = 0\n",
    "        missing_items = []\n",
    "\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                for minute in MINUTE_CHUNKS:\n",
    "                    name = f'{subject}-{question_i}-{minute}'\n",
    "                    if name not in return_dict:\n",
    "                        n_missing += 1\n",
    "                        missing_items.append((subject, question_i, minute, name))\n",
    "                    else:\n",
    "                        if return_dict[name][0] is None:\n",
    "                            n_missing += 1\n",
    "                            missing_items.append((subject, question_i, minute, name))\n",
    "\n",
    "        if n_missing == 0:\n",
    "            print(\"All responses successfully completed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Attempt {attempt}: {n_missing} / {total_requests} requests missing.\")\n",
    "\n",
    "        # Dispatch missing jobs\n",
    "        for (subject_i, (subject, question_i, minute, name)) in enumerate(missing_items):\n",
    "            #transcript = '\\n'.join([a for b in good_th_dict[subject] for a in b])\n",
    "            transcript, timestamps = clean_up_to_time(int(subject), minute)\n",
    "            proc = Process(target=multiproc_neo_wrapper, args=(transcript, question_i, name, return_dict, MODEL_TO_EVALUATE))\n",
    "            proc.start()\n",
    "            procs.append(proc)\n",
    "            time.sleep(sleep_between)\n",
    "\n",
    "            if len(procs) >= max_concurrent_calls:\n",
    "                for proc in procs:\n",
    "                    proc.join()\n",
    "                procs = []\n",
    "            \n",
    "                # Intermediate update\n",
    "                completed_items = dict(return_dict)\n",
    "                running_cost = np.sum([completed_items[x][-1] for x in completed_items])\n",
    "                n_completed = len(completed_items)\n",
    "                avg_cost = running_cost / n_completed if n_completed else 0\n",
    "                estimated_total_cost = avg_cost * total_requests\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time_per_call = elapsed_time / n_completed if n_completed else 0\n",
    "                remaining_calls = total_requests - n_completed\n",
    "                estimated_time_remaining = avg_time_per_call * remaining_calls\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Attempt {attempt}\")\n",
    "                print(f\"Completed: {n_completed}/{total_requests}\")\n",
    "                print(f\"Running cost: ${running_cost:.3f}\")\n",
    "                print(f\"Estimated total cost: ${estimated_total_cost:.3f}\")\n",
    "                print(f\"Elapsed time: {format_time(elapsed_time)}\")\n",
    "                print(f\"Estimated time remaining: {format_time(estimated_time_remaining)}\")\n",
    "        \n",
    "        # Final join\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "        running_cost = np.sum([return_dict[x][-1] for x in return_dict])\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Total running cost: {running_cost:.3f}\")\n",
    "        \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                for minute in MINUTE_CHUNKS:\n",
    "                    name = f'{subject}-{question_i}-{minute}'\n",
    "                    if name not in return_dict:\n",
    "                        n_missing += 1\n",
    "        completed_dict = dict(return_dict)\n",
    "        completed_items = [v for v in completed_dict.values()]\n",
    "        total_cost = np.sum([x[-1] for x in completed_items])\n",
    "        n_completed = len(completed_items)\n",
    "        avg_cost = total_cost / n_completed if n_completed else 0\n",
    "        total_elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Total responses expected: {total_requests}\")\n",
    "        print(f\"Successful: {n_completed}\")\n",
    "        print(f\"Failed: {n_missing}\")\n",
    "        print(f\"Total cost: ${total_cost:.3f}\")\n",
    "        print(f\"Avg cost per response: ${avg_cost:.4f}\")\n",
    "        print(f\"Total runtime: {format_time(total_elapsed_time)}\")\n",
    "        \n",
    "    else:\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                for minute in MINUTE_CHUNKS:\n",
    "                    name = f'{subject}-{question_i}-{minute}'\n",
    "                    if name not in return_dict:\n",
    "                        n_missing += 1\n",
    "                    else:\n",
    "                        if return_dict[name][0] is None:\n",
    "                            n_missing += 1\n",
    "                            missing_items.append((subject, question_i, minute, name))\n",
    "\n",
    "        print(f'!!! Max attempts reached. {n_missing} requests are still unresolved. !!!')\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae785f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T01:24:37.041316Z",
     "start_time": "2025-06-02T01:24:37.031211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_neo_ffi(df, minute):\n",
    "    # Scoring mapping\n",
    "    scoring_map = {'Strongly Disagree': 0, 'Disagree': 1, 'Neutral': 2, 'Agree': 3, 'Strongly Agree': 4}\n",
    "\n",
    "    # Negative scoring mapping (reverse scoring)\n",
    "    reverse_scoring_map = {'Strongly Disagree': 4, 'Disagree': 3, 'Neutral': 2, 'Agree': 1, 'Strongly Agree': 0}\n",
    "\n",
    "    # Define the items for each dimension based on the provided scoring guide\n",
    "    dimensions = {\n",
    "        'Neuroticism': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56],\n",
    "        'Extraversion': [2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 57],\n",
    "        'Openness': [3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 53, 58],\n",
    "        'Agreeableness': [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59],\n",
    "        'Conscientiousness': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "    }\n",
    "\n",
    "    # Reverse scoring items based on the provided scoring guide\n",
    "    reverse_items = [1, 16, 31, 46, 12, 27, 42, 57, 3, 8, 18, 23, 38, 48, 9, 14, 24, 29, 39, 44, 54, 59, 15, 30, 45, 55]\n",
    "\n",
    "    # Calculate scores for each dimension\n",
    "    scores = pd.DataFrame(df['subject'], columns=['subject'])\n",
    "    \n",
    "    for dimension, items in dimensions.items():\n",
    "        score = 0\n",
    "        for item in items:\n",
    "            column_name = f'question_{item}_{minute}'\n",
    "            if item in reverse_items:\n",
    "                unmapped = set(df[column_name]) - set(reverse_scoring_map)\n",
    "                if unmapped: raise ValueError(f\"Unmapped values found: {unmapped}\")\n",
    "                df[column_name] = df[column_name].map(reverse_scoring_map)\n",
    "            else:\n",
    "                unmapped = set(df[column_name]) - set(scoring_map)\n",
    "                if unmapped: raise ValueError(f\"Unmapped values found: {unmapped}\")\n",
    "                df[column_name] = df[column_name].map(scoring_map)\n",
    "        scores[dimension+f'_{minute}'] = df[[f'question_{item}_{minute}' for item in items]].sum(axis=1)\n",
    "        \n",
    "    return scores, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1c664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:40:38.291892Z",
     "start_time": "2025-06-02T01:24:38.185132Z"
    }
   },
   "outputs": [],
   "source": [
    "return_dict = run_until_resolved(good_th_dict, questions, MODEL_TO_EVALUATE)\n",
    "\n",
    "procs = []\n",
    "n_subs = len(good_th_dict)\n",
    "n_missing = 0\n",
    "\n",
    "for (subject_i, subject) in enumerate(good_th_dict):\n",
    "    transcript = '\\n'.join([a for b in good_th_dict[subject] for a in b])\n",
    "    for question_i in range(len(questions)):\n",
    "        for minute in MINUTE_CHUNKS:\n",
    "            name = f'{subject}-{question_i}-{minute}'\n",
    "            if name not in return_dict:\n",
    "                n_missing += 1\n",
    "\n",
    "assert n_missing == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf0dc1ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:42:49.456159Z",
     "start_time": "2025-06-02T02:42:48.697760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_dict = dict()\n",
    "for subject in good_th_dict:\n",
    "    for question_i in range(1, 61):\n",
    "        for minute in MINUTE_CHUNKS:\n",
    "            if subject not in out_dict:\n",
    "                out_dict[subject] = {}\n",
    "            out_dict[subject][f'question_{question_i}_{minute}'] = return_dict[f'{subject}-{question_i-1}-{minute}'][0]\n",
    "            if out_dict[subject][f'question_{question_i}_{minute}'] is None: raise Exception(f'{subject}-{question_i}-{minute} is None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9c72e87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:46:09.169005Z",
     "start_time": "2025-06-02T02:46:09.121875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_df = pd.DataFrame(out_dict).T\n",
    "all_qs = []\n",
    "for i in range(len(questions)):\n",
    "    for j in MINUTE_CHUNKS:\n",
    "        all_qs.append(f'question_{i+1}_{j}')\n",
    "out_df = out_df[all_qs]\n",
    "out_df['subject'] = pd.DataFrame(out_dict).T.index\n",
    "\n",
    "dfs = []\n",
    "for minute in MINUTE_CHUNKS:\n",
    "    gpt_neo_scores, rating_df = score_neo_ffi(out_df, minute)\n",
    "    gpt_neo_scores.columns = [x.lower() for x in gpt_neo_scores.columns]\n",
    "    dfs.append(gpt_neo_scores)\n",
    "\n",
    "gpt_neo_scores = pd.concat(dfs, axis=1)\n",
    "\n",
    "model_canonical_name = MODEL_TO_EVALUATE[-1]\n",
    "gpt_neo_scores.to_csv(f'{DATA_ROOT}/sst_{model_canonical_name}_text_per_question_scores_windowed.csv')\n",
    "out_df.to_csv(f'{DATA_ROOT}/sst_{model_canonical_name}_text_per_question_responses_windowed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3962c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174aeb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59582440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6950da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97bf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da19987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_testing",
   "language": "python",
   "name": "llm_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
