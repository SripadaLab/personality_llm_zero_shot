{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0f525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:14.299129Z",
     "start_time": "2025-07-23T04:10:14.287195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "def set_css_in_cell_output():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
    "            .widget-label {color: #d5d5d5 !important;}\n",
    "        </style>\n",
    "    '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css_in_cell_output)\n",
    "from IPython.core import ultratb\n",
    "ultratb.VerboseTB._tb_highlight = \"bg:#0D0D0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec3d23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:18.347702Z",
     "start_time": "2025-07-23T04:10:14.501360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tiktoken\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from jsonschema import validate\n",
    "from openai import OpenAI, RateLimitError, APITimeoutError, InternalServerError, Timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_incrementing, retry_if_exception_type, after_log, before_sleep_log\n",
    "import logging\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:18.693164Z",
     "start_time": "2025-07-23T04:10:18.350085Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = ''        # private openrouter api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c39d76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:22.309630Z",
     "start_time": "2025-06-11T15:07:22.305830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "request_limit_per_minute = 500\n",
    "token_limit_per_minute = 2e6\n",
    "\n",
    "request_timeout_seconds = 120   # maximum wait time for openAI to respond before triggering request timeout \n",
    "request_max_retries = 1         # number to times to automatically retry failed requests\n",
    "tpm_wait_polling_seconds = 10    # if our internal TPM estimate thinks TPM limit is exceeded, how often to check if limit cleared\n",
    "\n",
    "# global logger for static classes\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbfee26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:22.358487Z",
     "start_time": "2025-06-11T15:07:22.334869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ChatGPT:\n",
    "    def __init__(self, model_provider_order,\n",
    "                 halt_on_error=True,\n",
    "                 is_verbose=True,\n",
    "                 timeout=request_timeout_seconds,\n",
    "                 max_retries=request_max_retries,\n",
    "                 request_limit_per_minute=request_limit_per_minute,\n",
    "                 token_limit_per_minute=token_limit_per_minute,\n",
    "                 tpm_wait_polling_seconds=tpm_wait_polling_seconds,\n",
    "                 logger=logger,\n",
    "                 api_key=OPENROUTER_API_KEY,\n",
    "                 limit_manager_db_password=LIMIT_MANAGER_DB_PASSWORD):\n",
    "        self.model, self.provider_order, self.model_canonical_name = model_provider_order\n",
    "        self.halt_on_error = halt_on_error\n",
    "        self.is_verbose = is_verbose\n",
    "        self.tpm_wait_polling_seconds = tpm_wait_polling_seconds\n",
    "        self.request_limit_per_minute = request_limit_per_minute\n",
    "        self.request_delay_seconds = 60.0 / request_limit_per_minute\n",
    "        self.token_limit_per_minute = token_limit_per_minute\n",
    "        self.response_history = []\n",
    "        self.message_history = {}\n",
    "        self.logger = logger\n",
    "        self.limit_manager_db_password = limit_manager_db_password\n",
    "        likert_options = [\n",
    "            \"Strongly Disagree\",\n",
    "            \"Strongly Agree\",\n",
    "            \"Disagree\",\n",
    "            \"Neutral\",\n",
    "            \"Agree\",\n",
    "        ]\n",
    "        # Sort by length to match longer options (e.g., \"Strongly Agree\") before shorter ones (e.g., \"Agree\")\n",
    "        self.likert_options = sorted(likert_options, key=len, reverse=True)\n",
    "        self.default_seed = 1 if 'x-ai' in self.model else 0\n",
    "        \n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\",\n",
    "                             api_key = api_key,\n",
    "                             timeout=timeout,\n",
    "                             max_retries=max_retries)\n",
    "\n",
    "        \n",
    "    def extract_likert_response(self, content):\n",
    "        content_lower = content.lower()\n",
    "        for option in self.likert_options:\n",
    "            pattern = r'\\b' + re.escape(option.lower()) + r'\\b'\n",
    "            match = re.search(pattern, content_lower)\n",
    "            if match:\n",
    "                return json.dumps({\"response\": option})\n",
    "        raise Exception(\"No Likert match found in: \", content)    \n",
    "    \n",
    "    \n",
    "    def get_running_cost_num_prompt_completion_tokens(self):\n",
    "        \"\"\"\n",
    "        This function computes the total cost (estimated) of all\n",
    "        messages sent by the instance of ChatGPT called from\n",
    "        Returns: total_running_cost, total_num_prompt_tokens, total_num_response_tokens\n",
    "        \"\"\"\n",
    "        n_prompt_tokens = np.sum([x.usage.prompt_tokens for x in self.response_history])\n",
    "        n_completion_tokens = np.sum([x.usage.completion_tokens for x in self.response_history])\n",
    "        total_cost = sum(r.usage.cost for r in self.response_history)\n",
    "        return (total_cost,\n",
    "                n_prompt_tokens,\n",
    "                n_completion_tokens)\n",
    "\n",
    "    def get_key_usage_credits(self):\n",
    "        # get what OpenRouter says the api key has used in total\n",
    "        # returns usage, total credits available\n",
    "        resp = requests.get(\n",
    "            \"https://openrouter.ai/api/v1/credits\",\n",
    "            headers={\"Authorization\": f\"Bearer {self.client.api_key}\"}\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        info = resp.json()[\"data\"]\n",
    "        return info[\"total_usage\"], info[\"total_credits\"]\n",
    "\n",
    "    # retry failing requests starting with 10 second wait,\n",
    "    # increasing wait time by 10 seconds each retry, up to a max window of 120s (or 5 times)\n",
    "    # the goal is to try to avoid hitting backoff,\n",
    "    # we treat this as a last resort because of its runtime cost\n",
    "    @retry(wait=wait_incrementing(start=10, increment=10, max=120),\n",
    "           stop=stop_after_attempt(5),\n",
    "           retry=retry_if_exception_type((RateLimitError, APITimeoutError, InternalServerError, Timeout)),\n",
    "           before_sleep=before_sleep_log(logger, logging.INFO),\n",
    "           after=after_log(logger, logging.INFO))\n",
    "    def completion_with_backoff(self, client, **kwargs):\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "    def check_internal_TPM_tracker(self, n_message_tokens):\n",
    "        \"\"\"\n",
    "        Checks internal TPM count to see if a message with length = n_message_tokens\n",
    "        can be sent. If not, it waits (sleeps - blocking) until the message delivery\n",
    "        meets into TPM limit\n",
    "        \"\"\"\n",
    "        now = datetime.datetime.now()\n",
    "        one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "        self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "        n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        # fixed delay waiting if TPM exceeded over past minute\n",
    "        # this is cpu polling, so it doesnt cost money or much compute\n",
    "        while n_tokens_past_minute > self.token_limit_per_minute:\n",
    "            if self.is_verbose: self.logger.info(f'Internal TPM limit exceeded, waiting for {self.tpm_wait_polling_seconds} seconds...')\n",
    "            time.sleep(self.tpm_wait_polling_seconds)\n",
    "            now = datetime.datetime.now()\n",
    "            one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "            self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "            n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        now = datetime.datetime.now()\n",
    "        self.token_count_history.append((n_message_tokens, now))\n",
    "\n",
    "\n",
    "    def send_message(self, system_role, message, validate_response=True):\n",
    "        \"\"\"\n",
    "        This is the primary function used to send messages to GPT and get responses\n",
    "        Steps are:\n",
    "          - check that json schema meets our basic requirements\n",
    "          - handle RPM and TPM limits as best as we can\n",
    "            (when openai rejects requests for exceeding limits its much slower)\n",
    "          - build and send the message using openai ChatCompletion api\n",
    "          - perform basic validation on GPT's response\n",
    "        This function either returns a ChatCompletion response object or None (if failure occurred)\n",
    "        Errors are propogated using raised Exceptions\n",
    "        \"\"\"\n",
    "        # sleep based on RPM limit (lazy logic, avoids keeping running count of actual requests per minute)\n",
    "        time.sleep(self.request_delay_seconds)\n",
    "\n",
    "        # check TPM limit (not lazy, uses running count of tokens per minute)\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "        n_message_tokens = len(encoding.encode(system_role)) + len(encoding.encode(message))\n",
    "        self.logger.info(f'processing message with {n_message_tokens} tokens...')\n",
    "        if n_message_tokens > self.token_limit_per_minute:\n",
    "            return self.bad_response_output(f'Unable to send message as it exceeds TPM. Number of tokens in message = {n_message_tokens}')\n",
    "        \n",
    "        # build and send message over openai-api\n",
    "        message_id = len(self.message_history.keys())\n",
    "        self.message_history[message_id] = [] if 'x-ai' in self.model else [{\"role\": \"system\", \"content\": system_role}]\n",
    "        self.message_history[message_id].append({\"role\": \"user\", \"content\": message})\n",
    "        try:\n",
    "            response = self.completion_with_backoff(self.client,\n",
    "                                                    model=self.model,\n",
    "                                                    messages=self.message_history[message_id],\n",
    "                                                    temperature=0,\n",
    "                                                    stream=False,\n",
    "                                                    extra_body={\"usage\": {\"include\": True},\n",
    "                                                                \"reasoning\": {# One of the following (not both):\n",
    "                                                                              \"effort\": \"medium\", # Can be \"high\", \"medium\", or \"low\" (OpenAI-style)\n",
    "                                                                              # Optional: Default is false. All models support this.\n",
    "                                                                              \"exclude\": False # Set to true to exclude reasoning tokens from response\n",
    "                                                                              },\n",
    "                                                                \"provider\": {\"order\": self.provider_order, \n",
    "                                                                             \"sort\": \"price\",\n",
    "                                                                             \"data_collection\": \"deny\",\n",
    "                                                                             \"allow_fallbacks\": False}},\n",
    "                                                    seed=self.default_seed, logprobs=False)\n",
    "            \n",
    "            self.response_history.append(response)\n",
    "            # reasoning models dont return a content field\n",
    "            if response.choices[0].message.content is None:\n",
    "                self.bad_response_output(f'None in message content')\n",
    "                return None\n",
    "            elif response.choices[0].message.content == '':\n",
    "                if hasattr(response.choices[0].message, 'reasoning'):\n",
    "                    if response.choices[0].message.reasoning != '':\n",
    "                        response.choices[0].message.content = response.choices[0].message.reasoning\n",
    "            else:\n",
    "                pass\n",
    "            try:\n",
    "                likert_response = self.extract_likert_response(response.choices[0].message.content.strip().lower())\n",
    "                response.choices[0].message.content = likert_response\n",
    "            except:\n",
    "                self.bad_response_output(f'GPT response didnt match likert options')\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if self.halt_on_error:\n",
    "                raise\n",
    "            else:\n",
    "                if self.is_verbose:\n",
    "                    str_e = str(e)\n",
    "                    self.logger.info(f'An exception occurred: {str_e}')\n",
    "                    self.logger.info(traceback.format_exc())\n",
    "                return None\n",
    "\n",
    "        return (response, message_id)\n",
    "        \n",
    "\n",
    "    def bad_response_output(self, error):\n",
    "        # general function for informing the user when an error occurs\n",
    "        if self.halt_on_error:\n",
    "            raise Exception(error)\n",
    "        else:\n",
    "            if self.is_verbose:\n",
    "                self.logger.info(f'Error - {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c0808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:22.404145Z",
     "start_time": "2025-06-11T15:07:22.399539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ac189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5aa97a4",
   "metadata": {},
   "source": [
    "# Example: We expect a simple string response from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48583692",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_EVALUATE = gpt_41_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95702c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:26.608886Z",
     "start_time": "2025-07-23T04:10:26.602037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def example_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    response, message_history_id = None, -1\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message, \n",
    "                                                         validate_response=True)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        response_str = response_json[\"response\"]\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        response_str = ''\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    last_messages_sent_to_gpt = '' if (message_history_id not in chat.message_history) else chat.message_history[message_history_id]\n",
    "    print(f'Messages to GPT:\\n{last_messages_sent_to_gpt}')\n",
    "    print(f'Response from GPT:\\n{response_str}')\n",
    "    print(f'Cost: ${cost:.5f}')\n",
    "    \n",
    "    return response, message_history_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aeeb7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:27.233050Z",
     "start_time": "2025-06-11T15:07:26.196719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 271 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages to GPT:\n",
      "[{'role': 'system', 'content': 'you are a helpful assistant.'}, {'role': 'user', 'content': \"\\nSimulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\\n\\n---\\n\\nYour task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\\nBase your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\\n\\nFor each question, select the most appropriate option:\\n- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\\n- Disagree: The statement is mostly false or the participant would generally disagree with it.\\n- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\\n- Agree: The statement is mostly true or the participant would generally agree with it.\\n- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\\n\\nNEO-FFI question to answer:\\n{question}\\n\\nParticipant's spontaneous stream of thoughts:\\n{thoughts}\\n\\nYour response must be exactly one of:\\nStrongly Disagree  \\nDisagree  \\nNeutral  \\nAgree  \\nStrongly Agree\\n\\nDo not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\\n\"}]\n",
      "Response from GPT:\n",
      "Agree\n",
      "Cost: $0.00011\n"
     ]
    }
   ],
   "source": [
    "# specify system role and user message\n",
    "system_role = 'you are a helpful assistant.'\n",
    "likert_options = [\n",
    "    \"Strongly Disagree\",\n",
    "    \"Strongly Agree\",\n",
    "    \"Disagree\",\n",
    "    \"Neutral\",\n",
    "    \"Agree\",\n",
    "]\n",
    "likert_options_str = '\\n'.join(likert_options)\n",
    "\n",
    "message = \"\"\"\n",
    "Simulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\n",
    "\n",
    "---\n",
    "\n",
    "Your task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Disagree: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Agree: The statement is mostly true or the participant would generally agree with it.\n",
    "- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "NEO-FFI question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's spontaneous stream of thoughts:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Strongly Disagree  \n",
    "Disagree  \n",
    "Neutral  \n",
    "Agree  \n",
    "Strongly Agree\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "\n",
    "# create a single instance of ChatGPT \n",
    "# so that we can keep track of running costs\n",
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "response, message_history_id = example_messaging_wrapper(chat, system_role, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09d979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b01098",
   "metadata": {},
   "source": [
    "# SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec27c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:33.829612Z",
     "start_time": "2025-07-23T04:10:30.504947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509eb846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:10:33.841894Z",
     "start_time": "2025-07-23T04:10:33.831527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = './data/SST/'  # please contact the authors for access to the data\n",
    "\n",
    "good_th_dict = pickle.load(open(f'{DATA_ROOT}/SST_data.pickle', 'rb'))\n",
    "print(len([x for x in good_th_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c24367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:32.562984Z",
     "start_time": "2025-06-11T15:07:32.556861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"I am not a worrier.\",\n",
    "    \"I like to have a lot of people around me.\",\n",
    "    \"I don’t like to waste my time daydreaming.\",\n",
    "    \"I try to be courteous to everyone I meet.\",\n",
    "    \"I keep my belongings neat and clean.\",\n",
    "    \"I often feel inferior to others.\",\n",
    "    \"I laugh easily.\",\n",
    "    \"Once I find the right way to do something I stick to it.\",\n",
    "    \"I often get into arguments with my family and co-workers.\",\n",
    "    \"I’m pretty good about pacing myself so as to get things done on time.\",\n",
    "    \"When I’m under a great deal of stress sometimes I feel like I’m going to pieces.\",\n",
    "    \"I don’t consider myself especially 'light-hearted.'\",\n",
    "    \"I am intrigued by the patterns I find in art and nature.\",\n",
    "    \"Some people think I’m selfish and egotistical.\",\n",
    "    \"I am not a very methodical person.\",\n",
    "    \"I rarely feel lonely or blue.\",\n",
    "    \"I really enjoy talking to people.\",\n",
    "    \"I believe letting students hear controversial speakers can only confuse and mislead them.\",\n",
    "    \"I would rather cooperate with others than compete with them.\",\n",
    "    \"I try to perform all the tasks assigned to me conscientiously.\",\n",
    "    \"I often feel tense and jittery.\",\n",
    "    \"I like to be where the action is.\",\n",
    "    \"Poetry has little or no effect on me.\",\n",
    "    \"I tend to be cynical and skeptical of others’ intentions.\",\n",
    "    \"I have a clear set of goals and work toward them in an orderly fashion.\",\n",
    "    \"Sometimes I feel completely worthless.\",\n",
    "    \"I usually prefer to do things alone.\",\n",
    "    \"I often try new and foreign foods.\",\n",
    "    \"I believe that most people will take advantage of you if you let them.\",\n",
    "    \"I waste a lot of time before settling down to work.\",\n",
    "    \"I rarely feel fearful or anxious.\",\n",
    "    \"I often feel as if I’m bursting with energy.\",\n",
    "    \"I seldom notice the moods or feelings that different environments produce.\",\n",
    "    \"Most people I know like me.\",\n",
    "    \"I work hard to accomplish my goals.\",\n",
    "    \"I often get angry at the way people treat me.\",\n",
    "    \"I am a cheerful high-spirited person.\",\n",
    "    \"I believe we should look to our religious authorities for decisions on moral issues.\",\n",
    "    \"Some people think of me as cold and calculating.\",\n",
    "    \"When I make a commitment I can always be counted on to follow through.\",\n",
    "    \"Too often when things go wrong I get discouraged and feel like giving up.\",\n",
    "    \"I am not a cheerful optimist.\",\n",
    "    \"Sometimes when I am reading poetry or looking at a work of art I feel a chill or wave of excitement.\",\n",
    "    \"I’m hard-headed and tough-minded in my attitudes.\",\n",
    "    \"Sometimes I’m not as dependable or reliable as I should be.\",\n",
    "    \"I am seldom sad or depressed.\",\n",
    "    \"My life is fast-paced.\",\n",
    "    \"I have little interest in speculating on the nature of the universe or the human condition.\",\n",
    "    \"I generally try to be thoughtful and considerate.\",\n",
    "    \"I am a productive person who always gets the job done.\",\n",
    "    \"I often feel helpless and want someone else to solve my problems.\",\n",
    "    \"I am a very active person.\",\n",
    "    \"I have a lot of intellectual curiosity.\",\n",
    "    \"If I don’t like people I let them know it.\",\n",
    "    \"I never seem to be able to get organized.\",\n",
    "    \"At times I have been so ashamed I just wanted to hide.\",\n",
    "    \"I would rather go my own way than be a leader of others.\",\n",
    "    \"I often enjoy playing with theories or abstract ideas.\",\n",
    "    \"If necessary I am willing to manipulate people to get what I want.\",\n",
    "    \"I strive for excellence in everything I do.\"\n",
    "]\n",
    "for i in range(len(questions)):\n",
    "    questions[i] = f'Question {i+1}: ' + questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab56af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:34.222589Z",
     "start_time": "2025-06-11T15:07:34.218405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_NEO_FFI_prompt(thoughts, question):\n",
    "    prompt_template = f\"\"\"\n",
    "Your task is to respond to the following NEO-FFI question based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Disagree: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Agree: The statement is mostly true or the participant would generally agree with it.\n",
    "- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "NEO-FFI question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's spontaneous stream of thoughts:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Strongly Disagree  \n",
    "Disagree  \n",
    "Neutral  \n",
    "Agree  \n",
    "Strongly Agree\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7db536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:36.454135Z",
     "start_time": "2025-06-11T15:07:36.448995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generic_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    message_history_id = -1\n",
    "    required_values = None\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        required_values = response_json['response']\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    return required_values, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3972fa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:37.082055Z",
     "start_time": "2025-06-11T15:07:37.078737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_role = ''\n",
    "print(system_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef47f6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:38.940971Z",
     "start_time": "2025-06-11T15:07:38.204181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 4456 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disagree\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "thoughts = '\\n'.join([a for b in good_th_dict['5'] for a in b])\n",
    "\n",
    "question_i = 0\n",
    "question = questions[question_i]\n",
    "message = get_NEO_FFI_prompt(thoughts, question)\n",
    "required_values, cost = generic_messaging_wrapper(chat, system_role, message)\n",
    "\n",
    "print(required_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d61c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:38.951384Z",
     "start_time": "2025-06-11T15:07:38.942476Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def multiproc_neo_wrapper(thoughts, question_i, name, return_dict, model_to_evaluate):\n",
    "    try:\n",
    "        chat = ChatGPT(model_provider_order=model_to_evaluate)\n",
    "        question = questions[question_i]\n",
    "        message = get_NEO_FFI_prompt(thoughts, question)\n",
    "        required_values, cost = generic_messaging_wrapper(chat, system_role, message)\n",
    "        return_dict[f'{name}'] = (required_values, cost)\n",
    "    except Exception as e:\n",
    "        pass  # silent failures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23471078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:39.703134Z",
     "start_time": "2025-06-11T15:07:39.698848Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft limit: 4096 Hard limit: 262144\n",
      "New Soft limit: 30000 New Hard limit: 262144\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"Soft limit:\", soft, \"Hard limit:\", hard)\n",
    "# Raise soft limit (if you have permission):\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (30000, hard))\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"New Soft limit:\", soft, \"New Hard limit:\", hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1bec56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:41.158269Z",
     "start_time": "2025-06-11T15:07:41.141397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_until_resolved(good_th_dict, questions, MODEL_TO_EVALUATE,\n",
    "                       max_attempts=5, sleep_between=0.1, max_concurrent_calls=1000):\n",
    "    \n",
    "    return_dict = Manager().dict()\n",
    "    \n",
    "    n_subs = len(good_th_dict)\n",
    "    total_requests = len(good_th_dict) * len(questions)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def format_time(seconds):\n",
    "        mins, secs = divmod(int(seconds), 60)\n",
    "        hrs, mins = divmod(mins, 60)\n",
    "        return f\"{hrs:02d}:{mins:02d}:{secs:02d}\"\n",
    "    \n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        procs = []\n",
    "        n_missing = 0\n",
    "        missing_items = []\n",
    "\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "                    missing_items.append((subject, question_i, name))\n",
    "                else:\n",
    "                    if return_dict[name][0] is None:\n",
    "                        n_missing += 1\n",
    "                        missing_items.append((subject, question_i, name))\n",
    "\n",
    "        if n_missing == 0:\n",
    "            print(\"All responses successfully completed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Attempt {attempt}: {n_missing} / {total_requests} requests missing.\")\n",
    "\n",
    "        # Dispatch missing jobs\n",
    "        for (subject_i, (subject, question_i, name)) in enumerate(missing_items):\n",
    "            transcript = '\\n'.join([a for b in good_th_dict[subject] for a in b])\n",
    "            proc = Process(target=multiproc_neo_wrapper, args=(transcript, question_i, name, return_dict, MODEL_TO_EVALUATE))\n",
    "            proc.start()\n",
    "            procs.append(proc)\n",
    "            time.sleep(sleep_between)\n",
    "\n",
    "            if len(procs) >= max_concurrent_calls:\n",
    "                for proc in procs:\n",
    "                    proc.join()\n",
    "                procs = []\n",
    "            \n",
    "                # Intermediate update\n",
    "                completed_items = dict(return_dict)\n",
    "                running_cost = np.sum([completed_items[x][-1] for x in completed_items])\n",
    "                n_completed = len(completed_items)\n",
    "                avg_cost = running_cost / n_completed if n_completed else 0\n",
    "                estimated_total_cost = avg_cost * total_requests\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time_per_call = elapsed_time / n_completed if n_completed else 0\n",
    "                remaining_calls = total_requests - n_completed\n",
    "                estimated_time_remaining = avg_time_per_call * remaining_calls\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Attempt {attempt}\")\n",
    "                print(f\"Completed: {n_completed}/{total_requests}\")\n",
    "                print(f\"Running cost: ${running_cost:.3f}\")\n",
    "                print(f\"Estimated total cost: ${estimated_total_cost:.3f}\")\n",
    "                print(f\"Elapsed time: {format_time(elapsed_time)}\")\n",
    "                print(f\"Estimated time remaining: {format_time(estimated_time_remaining)}\")\n",
    "        \n",
    "        # Final join\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "        running_cost = np.sum([return_dict[x][-1] for x in return_dict])\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Total running cost: {running_cost:.3f}\")\n",
    "        \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "        completed_dict = dict(return_dict)\n",
    "        completed_items = [v for v in completed_dict.values()]\n",
    "        total_cost = np.sum([x[-1] for x in completed_items])\n",
    "        n_completed = len(completed_items)\n",
    "        avg_cost = total_cost / n_completed if n_completed else 0\n",
    "        total_elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Total responses expected: {total_requests}\")\n",
    "        print(f\"Successful: {n_completed}\")\n",
    "        print(f\"Failed: {n_missing}\")\n",
    "        print(f\"Total cost: ${total_cost:.3f}\")\n",
    "        print(f\"Avg cost per response: ${avg_cost:.4f}\")\n",
    "        print(f\"Total runtime: {format_time(total_elapsed_time)}\")\n",
    "        \n",
    "    else:\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "                else:\n",
    "                    if return_dict[name][0] is None:\n",
    "                        n_missing += 1\n",
    "                        missing_items.append((subject, question_i, name))\n",
    "\n",
    "        print(f'!!! Max attempts reached. {n_missing} requests are still unresolved. !!!')\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebd07e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:07:47.602388Z",
     "start_time": "2025-06-11T15:07:47.593350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_neo_ffi(df):\n",
    "    # Scoring mapping\n",
    "    scoring_map = {'Strongly Disagree': 0, 'Disagree': 1, 'Neutral': 2, 'Agree': 3, 'Strongly Agree': 4}\n",
    "\n",
    "    # Negative scoring mapping (reverse scoring)\n",
    "    reverse_scoring_map = {'Strongly Disagree': 4, 'Disagree': 3, 'Neutral': 2, 'Agree': 1, 'Strongly Agree': 0}\n",
    "\n",
    "    # Define the items for each dimension based on the provided scoring guide\n",
    "    dimensions = {\n",
    "        'Neuroticism': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56],\n",
    "        'Extraversion': [2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 57],\n",
    "        'Openness': [3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 53, 58],\n",
    "        'Agreeableness': [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59],\n",
    "        'Conscientiousness': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "    }\n",
    "\n",
    "    # Reverse scoring items based on the provided scoring guide\n",
    "    reverse_items = [1, 16, 31, 46, 12, 27, 42, 57, 3, 8, 18, 23, 38, 48, 9, 14, 24, 29, 39, 44, 54, 59, 15, 30, 45, 55]\n",
    "\n",
    "    # Calculate scores for each dimension\n",
    "    scores = pd.DataFrame(df['subject'], columns=['subject'])\n",
    "    \n",
    "    for dimension, items in dimensions.items():\n",
    "        score = 0\n",
    "        for item in items:\n",
    "            column_name = f'question_{item}'\n",
    "            if item in reverse_items:\n",
    "                unmapped = set(df[column_name]) - set(reverse_scoring_map)\n",
    "                if unmapped: raise ValueError(f\"Unmapped values found: {unmapped}\")\n",
    "                df[column_name] = df[column_name].map(reverse_scoring_map)\n",
    "            else:\n",
    "                unmapped = set(df[column_name]) - set(scoring_map)\n",
    "                if unmapped: raise ValueError(f\"Unmapped values found: {unmapped}\")\n",
    "                df[column_name] = df[column_name].map(scoring_map)\n",
    "        scores[dimension] = df[[f'question_{item}' for item in items]].sum(axis=1)\n",
    "        \n",
    "    return scores, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6e1c664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:26:10.360452Z",
     "start_time": "2025-06-11T15:17:43.461121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total responses expected: 3600\n",
      "Successful: 3600\n",
      "Failed: 0\n",
      "Total cost: $5.321\n",
      "Avg cost per response: $0.0015\n",
      "Total runtime: 00:08:26\n",
      "All responses successfully completed.\n"
     ]
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')\n",
    "\n",
    "\n",
    "for MODEL_TO_EVALUATE in [llama_4_maverick,\n",
    "                          gemini_flash,\n",
    "                          qwen_235B,\n",
    "                          gpt_41,\n",
    "                          gpt_41_mini,\n",
    "                          claude_sonnet,\n",
    "                          grok_3\n",
    "                          ]:\n",
    "\n",
    "    \n",
    "    return_dict = run_until_resolved(good_th_dict, questions, MODEL_TO_EVALUATE)\n",
    "\n",
    "    procs = []\n",
    "    n_subs = len(good_th_dict)\n",
    "    n_missing = 0\n",
    "\n",
    "    for (subject_i, subject) in enumerate(good_th_dict):\n",
    "        transcript = '\\n'.join([a for b in good_th_dict[subject] for a in b])\n",
    "        for question_i in range(len(questions)):\n",
    "            name = f'{subject}-{question_i}'\n",
    "            if name not in return_dict:\n",
    "                n_missing += 1\n",
    "\n",
    "    assert n_missing == 0\n",
    "\n",
    "    out_dict = dict()\n",
    "    for subject in good_th_dict:\n",
    "        for question_i in range(1, 61):\n",
    "            if subject not in out_dict:\n",
    "                out_dict[subject] = {}\n",
    "            out_dict[subject][f'question_{question_i}'] = return_dict[f'{subject}-{question_i-1}'][0]\n",
    "            if out_dict[subject][f'question_{question_i}'] is None: raise Exception(f'{subject}-{question_i} is None')\n",
    "\n",
    "\n",
    "    out_df = pd.DataFrame(out_dict).T\n",
    "    out_df = out_df[[f'question_{i+1}' for i in range(len(questions))]]\n",
    "    out_df['subject'] = pd.DataFrame(out_dict).T.index\n",
    "\n",
    "    gpt_neo_scores, rating_df = score_neo_ffi(out_df)\n",
    "    gpt_neo_scores.columns = [x.lower() for x in gpt_neo_scores.columns]\n",
    "\n",
    "    scales_df = pd.read_csv(f'{DATA_ROOT}/SST_scales.csv')\n",
    "    scales_df = scales_df.merge(gpt_neo_scores, on='subject', how='outer')\n",
    "    cols_to_keep = ['subject'] + [x for x in scales_df.columns if '_x' in x] + [x for x in scales_df.columns if '_y' in x]\n",
    "    scales_df = scales_df[cols_to_keep].dropna()\n",
    "\n",
    "    model_canonical_name = MODEL_TO_EVALUATE[-1]\n",
    "    gpt_neo_scores.to_csv(f'{DATA_ROOT}/sst_{model_canonical_name}_text_per_question_scores.csv')\n",
    "\n",
    "    out_df = pd.DataFrame(out_dict).T\n",
    "    out_df = out_df[[f'question_{i+1}' for i in range(len(questions))]]\n",
    "    out_df['subject'] = pd.DataFrame(out_dict).T.index\n",
    "    out_df.to_csv(f'{DATA_ROOT}/sst_{model_canonical_name}_text_per_question_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59582440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d7633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3c0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97bf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da19987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_testing",
   "language": "python",
   "name": "llm_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
