{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0f525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:16:57.453840Z",
     "start_time": "2025-07-23T04:16:57.437668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "def set_css_in_cell_output():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
    "            .widget-label {color: #d5d5d5 !important;}\n",
    "        </style>\n",
    "    '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css_in_cell_output)\n",
    "from IPython.core import ultratb\n",
    "ultratb.VerboseTB._tb_highlight = \"bg:#0D0D0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec3d23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:16:59.439273Z",
     "start_time": "2025-07-23T04:16:58.408307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tiktoken\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from jsonschema import validate\n",
    "from openai import OpenAI, RateLimitError, APITimeoutError, InternalServerError, Timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_incrementing, retry_if_exception_type, after_log, before_sleep_log\n",
    "import logging\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:16:59.783599Z",
     "start_time": "2025-07-23T04:16:59.440989Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = ''        # private openrouter api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c39d76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:57:34.927065Z",
     "start_time": "2025-06-02T02:57:34.923088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "request_limit_per_minute = 500\n",
    "token_limit_per_minute = 2e6\n",
    "\n",
    "request_timeout_seconds = 120   # maximum wait time for openAI to respond before triggering request timeout \n",
    "request_max_retries = 1         # number to times to automatically retry failed requests\n",
    "tpm_wait_polling_seconds = 10    # if our internal TPM estimate thinks TPM limit is exceeded, how often to check if limit cleared\n",
    "\n",
    "# global logger for static classes\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbfee26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:57:42.022530Z",
     "start_time": "2025-06-02T02:57:41.998569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ChatGPT:\n",
    "    def __init__(self, model_provider_order,\n",
    "                 halt_on_error=True,\n",
    "                 is_verbose=True,\n",
    "                 timeout=request_timeout_seconds,\n",
    "                 max_retries=request_max_retries,\n",
    "                 request_limit_per_minute=request_limit_per_minute,\n",
    "                 token_limit_per_minute=token_limit_per_minute,\n",
    "                 tpm_wait_polling_seconds=tpm_wait_polling_seconds,\n",
    "                 logger=logger,\n",
    "                 api_key=OPENROUTER_API_KEY,\n",
    "                 limit_manager_db_password=LIMIT_MANAGER_DB_PASSWORD):\n",
    "        self.model, self.provider_order, self.model_canonical_name = model_provider_order\n",
    "        self.halt_on_error = halt_on_error\n",
    "        self.is_verbose = is_verbose\n",
    "        self.tpm_wait_polling_seconds = tpm_wait_polling_seconds\n",
    "        self.request_limit_per_minute = request_limit_per_minute\n",
    "        self.request_delay_seconds = 60.0 / request_limit_per_minute\n",
    "        self.token_limit_per_minute = token_limit_per_minute\n",
    "        self.response_history = []\n",
    "        self.message_history = {}\n",
    "        self.logger = logger\n",
    "        self.limit_manager_db_password = limit_manager_db_password\n",
    "        likert_options = [\n",
    "            \"Very Inaccurate\",\n",
    "            \"Moderately Inaccurate\",\n",
    "            \"Neither Accurate nor Inaccurate\",\n",
    "            \"Moderately Accurate\",\n",
    "            \"Very Accurate\",\n",
    "        ]\n",
    "        # Sort by length to match longer options (e.g., \"Strongly Agree\") before shorter ones (e.g., \"Agree\")\n",
    "        self.likert_options = sorted(likert_options, key=len, reverse=True)\n",
    "        self.default_seed = 1 if 'x-ai' in self.model else 0\n",
    "        \n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\",\n",
    "                             api_key = api_key,\n",
    "                             timeout=timeout,\n",
    "                             max_retries=max_retries)\n",
    "\n",
    "        \n",
    "    def extract_likert_response(self, content):\n",
    "        content_lower = content.lower()\n",
    "        for option in self.likert_options:\n",
    "            pattern = r'\\b' + re.escape(option.lower()) + r'\\b'\n",
    "            match = re.search(pattern, content_lower)\n",
    "            if match:\n",
    "                return json.dumps({\"response\": option})\n",
    "        raise Exception(\"No Likert match found in: \", content)    \n",
    "    \n",
    "    \n",
    "    def get_running_cost_num_prompt_completion_tokens(self):\n",
    "        \"\"\"\n",
    "        This function computes the total cost (estimated) of all\n",
    "        messages sent by the instance of ChatGPT called from\n",
    "        Returns: total_running_cost, total_num_prompt_tokens, total_num_response_tokens\n",
    "        \"\"\"\n",
    "        n_prompt_tokens = np.sum([x.usage.prompt_tokens for x in self.response_history])\n",
    "        n_completion_tokens = np.sum([x.usage.completion_tokens for x in self.response_history])\n",
    "        total_cost = sum(r.usage.cost for r in self.response_history)\n",
    "        return (total_cost,\n",
    "                n_prompt_tokens,\n",
    "                n_completion_tokens)\n",
    "\n",
    "    def get_key_usage_credits(self):\n",
    "        # get what OpenRouter says the api key has used in total\n",
    "        # returns usage, total credits available\n",
    "        resp = requests.get(\n",
    "            \"https://openrouter.ai/api/v1/credits\",\n",
    "            headers={\"Authorization\": f\"Bearer {self.client.api_key}\"}\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        info = resp.json()[\"data\"]\n",
    "        return info[\"total_usage\"], info[\"total_credits\"]\n",
    "\n",
    "    # retry failing requests starting with 10 second wait,\n",
    "    # increasing wait time by 10 seconds each retry, up to a max window of 120s (or 5 times)\n",
    "    # the goal is to try to avoid hitting backoff,\n",
    "    # we treat this as a last resort because of its runtime cost\n",
    "    @retry(wait=wait_incrementing(start=10, increment=10, max=120),\n",
    "           stop=stop_after_attempt(5),\n",
    "           retry=retry_if_exception_type((RateLimitError, APITimeoutError, InternalServerError, Timeout)),\n",
    "           before_sleep=before_sleep_log(logger, logging.INFO),\n",
    "           after=after_log(logger, logging.INFO))\n",
    "    def completion_with_backoff(self, client, **kwargs):\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "    def check_internal_TPM_tracker(self, n_message_tokens):\n",
    "        \"\"\"\n",
    "        Checks internal TPM count to see if a message with length = n_message_tokens\n",
    "        can be sent. If not, it waits (sleeps - blocking) until the message delivery\n",
    "        meets into TPM limit\n",
    "        \"\"\"\n",
    "        now = datetime.datetime.now()\n",
    "        one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "        self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "        n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        # fixed delay waiting if TPM exceeded over past minute\n",
    "        # this is cpu polling, so it doesnt cost money or much compute\n",
    "        while n_tokens_past_minute > self.token_limit_per_minute:\n",
    "            if self.is_verbose: self.logger.info(f'Internal TPM limit exceeded, waiting for {self.tpm_wait_polling_seconds} seconds...')\n",
    "            time.sleep(self.tpm_wait_polling_seconds)\n",
    "            now = datetime.datetime.now()\n",
    "            one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "            self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "            n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        now = datetime.datetime.now()\n",
    "        self.token_count_history.append((n_message_tokens, now))\n",
    "\n",
    "\n",
    "    def send_message(self, system_role, message, validate_response=True):\n",
    "        \"\"\"\n",
    "        This is the primary function used to send messages to GPT and get responses\n",
    "        Steps are:\n",
    "          - check that json schema meets our basic requirements\n",
    "          - handle RPM and TPM limits as best as we can\n",
    "            (when openai rejects requests for exceeding limits its much slower)\n",
    "          - build and send the message using openai ChatCompletion api\n",
    "          - perform basic validation on GPT's response\n",
    "        This function either returns a ChatCompletion response object or None (if failure occurred)\n",
    "        Errors are propogated using raised Exceptions\n",
    "        \"\"\"\n",
    "        # sleep based on RPM limit (lazy logic, avoids keeping running count of actual requests per minute)\n",
    "        time.sleep(self.request_delay_seconds)\n",
    "\n",
    "        # check TPM limit (not lazy, uses running count of tokens per minute)\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "        n_message_tokens = len(encoding.encode(system_role)) + len(encoding.encode(message))\n",
    "        self.logger.info(f'processing message with {n_message_tokens} tokens...')\n",
    "        if n_message_tokens > self.token_limit_per_minute:\n",
    "            return self.bad_response_output(f'Unable to send message as it exceeds TPM. Number of tokens in message = {n_message_tokens}')\n",
    "        \n",
    "        # build and send message over openai-api\n",
    "        message_id = len(self.message_history.keys())\n",
    "        self.message_history[message_id] = [] if 'x-ai' in self.model else [{\"role\": \"system\", \"content\": system_role}]\n",
    "        self.message_history[message_id].append({\"role\": \"user\", \"content\": message})\n",
    "        try:\n",
    "            response = self.completion_with_backoff(self.client,\n",
    "                                                    model=self.model,\n",
    "                                                    messages=self.message_history[message_id],\n",
    "                                                    temperature=0,\n",
    "                                                    stream=False,\n",
    "                                                    extra_body={\"usage\": {\"include\": True},\n",
    "                                                                \"reasoning\": {# One of the following (not both):\n",
    "                                                                              \"effort\": \"medium\", # Can be \"high\", \"medium\", or \"low\" (OpenAI-style)\n",
    "                                                                              # Optional: Default is false. All models support this.\n",
    "                                                                              \"exclude\": False # Set to true to exclude reasoning tokens from response\n",
    "                                                                              },\n",
    "                                                                \"provider\": {\"order\": self.provider_order, \n",
    "                                                                             \"sort\": \"price\",\n",
    "                                                                             \"data_collection\": \"deny\",\n",
    "                                                                             \"allow_fallbacks\": False}},\n",
    "                                                    seed=self.default_seed, logprobs=False)\n",
    "            \n",
    "            self.response_history.append(response)\n",
    "            # reasoning models dont return a content field\n",
    "            if response.choices[0].message.content is None:\n",
    "                self.bad_response_output(f'None in message content')\n",
    "                return None\n",
    "            elif response.choices[0].message.content == '':\n",
    "                if hasattr(response.choices[0].message, 'reasoning'):\n",
    "                    if response.choices[0].message.reasoning != '':\n",
    "                        response.choices[0].message.content = response.choices[0].message.reasoning\n",
    "            else:\n",
    "                pass\n",
    "            try:\n",
    "                likert_response = self.extract_likert_response(response.choices[0].message.content.strip().lower())\n",
    "                response.choices[0].message.content = likert_response\n",
    "            except:\n",
    "                self.bad_response_output(f'GPT response didnt match likert options')\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if self.halt_on_error:\n",
    "                raise\n",
    "            else:\n",
    "                if self.is_verbose:\n",
    "                    str_e = str(e)\n",
    "                    self.logger.info(f'An exception occurred: {str_e}')\n",
    "                    self.logger.info(traceback.format_exc())\n",
    "                return None\n",
    "\n",
    "        return (response, message_id)\n",
    "        \n",
    "\n",
    "    def bad_response_output(self, error):\n",
    "        # general function for informing the user when an error occurs\n",
    "        if self.halt_on_error:\n",
    "            raise Exception(error)\n",
    "        else:\n",
    "            if self.is_verbose:\n",
    "                self.logger.info(f'Error - {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c0808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:57:42.751195Z",
     "start_time": "2025-06-02T02:57:42.746319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ac189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5aa97a4",
   "metadata": {},
   "source": [
    "# Example: We expect a simple string response from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_EVALUATE = gemini_flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95702c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:57:43.936279Z",
     "start_time": "2025-06-02T02:57:43.930074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def example_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    response, message_history_id = None, -1\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message, \n",
    "                                                         validate_response=True)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        response_str = response_json[\"response\"]\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        response_str = ''\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    last_messages_sent_to_gpt = '' if (message_history_id not in chat.message_history) else chat.message_history[message_history_id]\n",
    "    print(f'Messages to GPT:\\n{last_messages_sent_to_gpt}')\n",
    "    print(f'Response from GPT:\\n{response_str}')\n",
    "    print(f'Cost: ${cost:.5f}')\n",
    "    \n",
    "    return response, message_history_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aeeb7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:57:49.173078Z",
     "start_time": "2025-06-02T02:57:44.406966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 307 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages to GPT:\n",
      "[{'role': 'system', 'content': 'you are a helpful assistant.'}, {'role': 'user', 'content': \"\\nSimulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\\n\\n---\\n\\nYour task is to respond to the following IPIP-NEO-120 question based on the participant's daily diaries of the most significant event that occurred during the day, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\\nBase your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\\n\\nFor each question, select the most appropriate option:\\n- Very Inaccurate: The statement is definitely false or the participant would strongly disagree with it.\\n- Moderately Inaccurate: The statement is mostly false or the participant would generally disagree with it.\\n- Neither Accurate nor Inaccurate: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\\n- Moderately Accurate: The statement is mostly true or the participant would generally agree with it.\\n- Very Accurate: The statement is definitely true or the participant would strongly agree with it.\\n\\nIPIP-NEO-120 question to answer:\\n{question}\\n\\nParticipant's daily diaries:\\n{thoughts}\\n\\nYour response must be exactly one of:\\nVery Inaccurate\\nModerately Inaccurate\\nNeither Accurate nor Inaccurate\\nModerately Accurate\\nVery Accurate\\n\\nDo not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\\n\"}]\n",
      "Response from GPT:\n",
      "Neither Accurate nor Inaccurate\n",
      "Cost: $0.00175\n"
     ]
    }
   ],
   "source": [
    "# specify system role and user message\n",
    "system_role = 'you are a helpful assistant.'\n",
    "likert_options = [\n",
    "    \"Strongly Disagree\",\n",
    "    \"Strongly Agree\",\n",
    "    \"Disagree\",\n",
    "    \"Neutral\",\n",
    "    \"Agree\",\n",
    "]\n",
    "likert_options_str = '\\n'.join(likert_options)\n",
    "\n",
    "message = \"\"\"\n",
    "Simulate your response to the prompt below without being provided the question and thoughts. This is for testing purposes.\n",
    "\n",
    "---\n",
    "\n",
    "Your task is to respond to the following IPIP-NEO-120 question based on the participant's daily diaries of the most significant event that occurred during the day, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Very Inaccurate: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Moderately Inaccurate: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neither Accurate nor Inaccurate: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Moderately Accurate: The statement is mostly true or the participant would generally agree with it.\n",
    "- Very Accurate: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "IPIP-NEO-120 question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's daily diaries:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Very Inaccurate\n",
    "Moderately Inaccurate\n",
    "Neither Accurate nor Inaccurate\n",
    "Moderately Accurate\n",
    "Very Accurate\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "\n",
    "# create a single instance of ChatGPT \n",
    "# so that we can keep track of running costs\n",
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "response, message_history_id = example_messaging_wrapper(chat, system_role, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09d979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b01098",
   "metadata": {},
   "source": [
    "# AAPECS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec27c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:17:07.352452Z",
     "start_time": "2025-07-23T04:17:06.471535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c24367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:17:07.380060Z",
     "start_time": "2025-07-23T04:17:07.354649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_list = [\n",
    "    {'text': 'Worry about things', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Make friends easily', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Have a vivid imagination', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Trust others', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Complete tasks successfully', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Get angry easily', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Love large parties', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Believe in the importance of art', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Use others for my own ends', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Like to tidy up', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Often feel blue', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Take charge', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Experience my emotions intensely', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Love to help others', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Keep my promises', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Find it difficult to approach others', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Am always busy', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Prefer variety to routine', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Love a good fight', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Work hard', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Go on binges', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Love excitement', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Love to read challenging material', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Believe that I am better than others', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Am always prepared', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Panic easily', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Radiate joy', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Tend to vote for liberal political candidates', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Sympathize with the homeless', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Jump into things without thinking', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Fear for the worst', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Feel comfortable around people', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Enjoy wild flights of fantasy', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Believe that others have good intentions', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Excel in what I do', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Get irritated easily', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Talk to a lot of different people at parties', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'See beauty in things that others might not notice', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Cheat to get ahead', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Often forget to put things back in their proper place', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Dislike myself', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Try to lead others', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Feel others\\' emotions', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Am concerned about others', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Tell the truth', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Am afraid to draw attention to myself', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Am always on the go', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Prefer to stick with things that I know', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Yell at people', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Do more than what\\'s expected of me', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Rarely overindulge', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Seek adventure', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Avoid philosophical discussions', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Think highly of myself', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Carry out my plans', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Become overwhelmed by events', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Have a lot of fun', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Believe that there is no absolute right and wrong', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Feel sympathy for those who are worse off than myself', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Make rash decisions', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Am afraid of many things', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Avoid contacts with others', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': 'Love to daydream', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Trust what people say', 'keyed': 'plus', 'domain': 'A'},\n",
    "    {'text': 'Handle tasks smoothly', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Lose my temper', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Prefer to be alone', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': 'Do not like poetry', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Take advantage of others', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Leave a mess in my room', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Am often down in the dumps', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Take control of things', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Rarely notice my emotional reactions', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Am indifferent to the feelings of others', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Break rules', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Only feel comfortable with friends', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Do a lot in my spare time', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Dislike changes', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Insult people', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Do just enough work to get by', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Easily resist temptations', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Enjoy being reckless', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Have difficulty understanding abstract ideas', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Have a high opinion of myself', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Waste my time', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': \"Feel that I'm unable to deal with things\", 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Love life', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Tend to vote for conservative political candidates', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': \"Am not interested in other people's problems\", 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Rush into things', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Get stressed out easily', 'keyed': 'plus', 'domain': 'N'},\n",
    "    {'text': 'Keep others at a distance', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': 'Like to get lost in thought', 'keyed': 'plus', 'domain': 'O'},\n",
    "    {'text': 'Distrust people', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Know how to get things done', 'keyed': 'plus', 'domain': 'C'},\n",
    "    {'text': 'Am not easily annoyed', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Avoid crowds', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': 'Do not enjoy going to art museums', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': \"Obstruct others' plans\", 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Leave my belongings around', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Feel comfortable with myself', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Wait for others to lead the way', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': \"Don't understand people who get emotional\", 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Take no time for others', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Break my promises', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Am not bothered by difficult social situations', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Like to take it easy', 'keyed': 'minus', 'domain': 'E'},\n",
    "    {'text': 'Am attached to conventional ways', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Get back at others', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Put little time and effort into my work', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Am able to control my cravings', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Act wild and crazy', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Am not interested in theoretical discussions', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Boast about my virtues', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Have difficulty starting tasks', 'keyed': 'minus', 'domain': 'C'},\n",
    "    {'text': 'Remain calm under pressure', 'keyed': 'minus', 'domain': 'N'},\n",
    "    {'text': 'Look at the bright side of life', 'keyed': 'plus', 'domain': 'E'},\n",
    "    {'text': 'Believe that we should be tough on crime', 'keyed': 'minus', 'domain': 'O'},\n",
    "    {'text': 'Try not to think about the needy', 'keyed': 'minus', 'domain': 'A'},\n",
    "    {'text': 'Act without thinking', 'keyed': 'minus', 'domain': 'C'}\n",
    "]\n",
    "\n",
    "for i in range(len(questions_list)):\n",
    "    questions_list[i]['text'] = f'Question {i+1}: ' + questions_list[i]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987488fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './data/AAPECS/'  # please contact the authors for access to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be3f13fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:29:01.646539Z",
     "start_time": "2025-07-23T04:28:59.779989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b833b71f254ee4baf0a8337c0bf0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pheno_df = pd.read_csv(f'{DATA_ROOT}/eod_new_time.csv')\n",
    "data_root = f'{DATA_ROOT}/raw_video_logs'\n",
    "data_files = [f for f in listdir(data_root) if isfile(join(data_root, f))]\n",
    "\n",
    "sub_transcripts = {}\n",
    "sub_lengths = {}\n",
    "\n",
    "for file in tqdm(data_files):\n",
    "    df = pd.read_csv(f'{data_root}/{file}')\n",
    "    df = df[(df.values[:, -1] != 'NO_ANSWER') & (df.values[:, -1] != 'SKIPPED')]\n",
    "\n",
    "    dates = [x.replace('/', '_') for x in df['Survey Submitted Date'].values]  # dd_mm_yyyy\n",
    "    times = [x.replace(':', '-') for x in df['Survey Submitted Time'].values]  # dd_mm_yyyy\n",
    "    addresses = df.values[:, -1]\n",
    "    userid = df['User Id'].values\n",
    "    usernum = file.lower().replace('eod', '').replace('vids', '').replace('videos', '').replace('.csv', '').replace('video', '')\n",
    "    usernum = int(usernum)\n",
    "    triggers = [x.replace(' ', '') for x in df['Trigger Type'].values]\n",
    "\n",
    "    assert np.all(['http' in x for x in addresses])\n",
    "    assert np.all((df['Trigger Type'].values == \"DAILY\") | (df['Trigger Type'].values == \"DELETED TRIGGER\") | (df['Trigger Type'].values == \"ONCE\"))\n",
    "    assert np.all([x.split('/')[-1].split('.')][-1] == 'mp4' for x in addresses)\n",
    "    \n",
    "    sub_pheno_df = pheno_df[pheno_df.participantID == usernum]\n",
    "    if sub_pheno_df.shape[0] == 0: continue\n",
    "\n",
    "    \n",
    "    for i in range(len(addresses)):\n",
    "        txt = json.load(open(f'{DATA_ROOT}/transcripts_json/{usernum}/{usernum}_{dates[i]}_{times[i]}_{triggers[i]}_{userid[i]}.json'))['text'].strip()\n",
    "        \n",
    "        day, month, year = dates[i].split('_')\n",
    "        sub = usernum\n",
    "        if sub not in sub_transcripts:\n",
    "            sub_transcripts[sub] = []\n",
    "        sub_transcripts[sub].append(txt)\n",
    "        \n",
    "        recording_length = json.load(open(f'{DATA_ROOT}/transcripts_json/{usernum}/{usernum}_{dates[i]}_{times[i]}_{triggers[i]}_{userid[i]}.json'))['segments'][-1]['end']\n",
    "        if sub not in sub_lengths:\n",
    "            sub_lengths[sub] = []\n",
    "        sub_lengths[sub].append(recording_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab56af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:20.121782Z",
     "start_time": "2025-06-02T02:58:20.117704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_NEO_FFI_prompt(thoughts, question):\n",
    "    prompt_template = f\"\"\"\n",
    "Your task is to respond to the following IPIP-NEO-120 question based on the participant's daily diaries of the most significant event that occurred during the day, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Very Inaccurate: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Moderately Inaccurate: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neither Accurate nor Inaccurate: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Moderately Accurate: The statement is mostly true or the participant would generally agree with it.\n",
    "- Very Accurate: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "IPIP-NEO-120 question to answer:\n",
    "{question}\n",
    "\n",
    "Participant's daily diaries:\n",
    "{thoughts}\n",
    "\n",
    "Your response must be exactly one of:\n",
    "Very Inaccurate\n",
    "Moderately Inaccurate\n",
    "Neither Accurate nor Inaccurate\n",
    "Moderately Accurate\n",
    "Very Accurate\n",
    "\n",
    "Do not include any explanation, punctuation, or additional text. Return only the exact phrase from the list above.\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7db536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:20.128644Z",
     "start_time": "2025-06-02T02:58:20.123747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generic_messaging_wrapper(chat, system_role, message):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    message_history_id = -1\n",
    "    required_values = None\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        required_values = response_json['response']\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    return required_values, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3972fa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:20.133013Z",
     "start_time": "2025-06-02T02:58:20.129826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_role = ''\n",
    "print(system_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef47f6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:28.618023Z",
     "start_time": "2025-06-02T02:58:20.134271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 3036 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Accurate\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "thoughts = '\\n'.join(sub_transcripts[54])\n",
    "domain_questions = [x['text'] for x in questions_list if x['domain']=='N']\n",
    "message = get_NEO_FFI_prompt(thoughts, domain_questions[0])\n",
    "required_values, cost = generic_messaging_wrapper(chat, system_role, message)\n",
    "\n",
    "print(required_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8abbef6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:28.634706Z",
     "start_time": "2025-06-02T02:58:28.630302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Process, Manager\n",
    "\n",
    "def multiproc_neo_wrapper(thoughts, question_i, name, return_dict, model_to_evaluate):\n",
    "    try:\n",
    "        chat = ChatGPT(model_provider_order=model_to_evaluate)\n",
    "        dim_question = questions_list[question_i]['text']\n",
    "        message = get_NEO_FFI_prompt(thoughts, dim_question)\n",
    "        required_values, cost = generic_messaging_wrapper(chat, system_role, message)\n",
    "        return_dict[f'{name}'] = (required_values, cost)\n",
    "    except Exception as e:\n",
    "        pass  # silent failures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ae67f7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:28.682873Z",
     "start_time": "2025-06-02T02:58:28.674998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft limit: 4096 Hard limit: 262144\n",
      "New Soft limit: 30000 New Hard limit: 262144\n"
     ]
    }
   ],
   "source": [
    "import resource\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"Soft limit:\", soft, \"Hard limit:\", hard)\n",
    "# Raise soft limit (if you have permission):\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (30000, hard))\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "print(\"New Soft limit:\", soft, \"New Hard limit:\", hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b1bec56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T02:58:46.126726Z",
     "start_time": "2025-06-02T02:58:46.109151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_until_resolved(good_th_dict, questions, return_dict, model_to_evaluate,\n",
    "                       max_attempts=5, sleep_between=0.1, max_concurrent_calls=7500):\n",
    "    return_dict = Manager().dict()\n",
    "    \n",
    "    n_subs = len(good_th_dict)\n",
    "    total_requests = len(good_th_dict) * len(questions)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def format_time(seconds):\n",
    "        mins, secs = divmod(int(seconds), 60)\n",
    "        hrs, mins = divmod(mins, 60)\n",
    "        return f\"{hrs:02d}:{mins:02d}:{secs:02d}\"\n",
    "    \n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        procs = []\n",
    "        n_missing = 0\n",
    "        missing_items = []\n",
    "\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "                    missing_items.append((subject, question_i, name))\n",
    "                else:\n",
    "                    if return_dict[name][0] is None:\n",
    "                        n_missing += 1\n",
    "                        missing_items.append((subject, question_i, name))\n",
    "\n",
    "        if n_missing == 0:\n",
    "            print(\"All responses successfully completed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Attempt {attempt}: {n_missing} / {total_requests} requests missing.\")\n",
    "\n",
    "        # Dispatch missing jobs\n",
    "        for (subject_i, (subject, question_i, name)) in enumerate(missing_items):\n",
    "            transcript = '\\n'.join([x.replace('\\n', '') for x in sub_transcripts[subject]])\n",
    "            proc = Process(target=multiproc_neo_wrapper, args=(transcript, question_i, name, return_dict, model_to_evaluate))\n",
    "            proc.start()\n",
    "            procs.append(proc)\n",
    "            time.sleep(sleep_between)\n",
    "\n",
    "            if len(procs) >= max_concurrent_calls:\n",
    "                for proc in procs:\n",
    "                    proc.join()\n",
    "                procs = []\n",
    "            \n",
    "                # Intermediate update\n",
    "                completed_items = dict(return_dict)\n",
    "                running_cost = np.sum([completed_items[x][-1] for x in completed_items])\n",
    "                n_completed = len(completed_items)\n",
    "                avg_cost = running_cost / n_completed if n_completed else 0\n",
    "                estimated_total_cost = avg_cost * total_requests\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time_per_call = elapsed_time / n_completed if n_completed else 0\n",
    "                remaining_calls = total_requests - n_completed\n",
    "                estimated_time_remaining = avg_time_per_call * remaining_calls\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Attempt {attempt}\")\n",
    "                print(f\"Completed: {n_completed}/{total_requests}\")\n",
    "                print(f\"Running cost: ${running_cost:.3f}\")\n",
    "                print(f\"Estimated total cost: ${estimated_total_cost:.3f}\")\n",
    "                print(f\"Elapsed time: {format_time(elapsed_time)}\")\n",
    "                print(f\"Estimated time remaining: {format_time(estimated_time_remaining)}\")\n",
    "                \n",
    "        # Final join\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "        running_cost = np.sum([return_dict[x][-1] for x in return_dict])\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Total running cost: {running_cost:.3f}\")\n",
    "        \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "        completed_dict = dict(return_dict)\n",
    "        completed_items = [v for v in completed_dict.values()]\n",
    "        total_cost = np.sum([x[-1] for x in completed_items])\n",
    "        n_completed = len(completed_items)\n",
    "        avg_cost = total_cost / n_completed if n_completed else 0\n",
    "        total_elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Total responses expected: {total_requests}\")\n",
    "        print(f\"Successful: {n_completed}\")\n",
    "        print(f\"Failed: {n_missing}\")\n",
    "        print(f\"Total cost: ${total_cost:.3f}\")\n",
    "        print(f\"Avg cost per response: ${avg_cost:.4f}\")\n",
    "        print(f\"Total runtime: {format_time(total_elapsed_time)}\")\n",
    "        \n",
    "    else:\n",
    "        n_missing = 0\n",
    "        # Check for missing entries\n",
    "        for subject in good_th_dict:\n",
    "            for question_i in range(len(questions)):\n",
    "                name = f'{subject}-{question_i}'\n",
    "                if name not in return_dict:\n",
    "                    n_missing += 1\n",
    "                else:\n",
    "                    if return_dict[name][0] is None:\n",
    "                        n_missing += 1\n",
    "                        missing_items.append((subject, question_i, name))\n",
    "\n",
    "        print(f'!!! Max attempts reached. {n_missing} requests are still unresolved. !!!')\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_responses_to_integers(response_list, reverse_coding_list):\n",
    "    # Define the mapping from response options to integers\n",
    "    response_mapping = {\n",
    "        'Very Inaccurate': 0,\n",
    "        'Moderately Inaccurate': 1,\n",
    "        'Neither Accurate nor Inaccurate': 2,\n",
    "        'Moderately Accurate': 3,\n",
    "        'Very Accurate': 4,\n",
    "        None: np.nan\n",
    "    }\n",
    "    \n",
    "    reverse_mapping = {\n",
    "        'Very Inaccurate': 4,\n",
    "        'Moderately Inaccurate': 3,\n",
    "        'Neither Accurate nor Inaccurate': 2,\n",
    "        'Moderately Accurate': 1,\n",
    "        'Very Accurate': 0,\n",
    "        None: np.nan\n",
    "    }\n",
    "\n",
    "    # Iterate through both lists and apply the appropriate mapping based on reverse coding\n",
    "    mapped_responses = [\n",
    "        reverse_mapping[response] if reverse_coding else response_mapping[response]\n",
    "        for response, reverse_coding in zip(response_list, reverse_coding_list)\n",
    "    ]\n",
    "\n",
    "    return mapped_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6e1c664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T03:23:44.478954Z",
     "start_time": "2025-06-02T02:59:16.385128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total responses expected: 12960\n",
      "Successful: 12960\n",
      "Failed: 0\n",
      "Total cost: $87.135\n",
      "Avg cost per response: $0.0067\n",
      "Total runtime: 00:24:27\n",
      "All responses successfully completed.\n"
     ]
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')\n",
    "\n",
    "\n",
    "for MODEL_TO_EVALUATE in [llama_4_maverick,\n",
    "                          qwen_235B,\n",
    "                          gpt_41,\n",
    "                          gpt_41_mini,\n",
    "                          claude_sonnet,\n",
    "                          grok_3,\n",
    "                          gemini_flash\n",
    "                          ]:\n",
    "\n",
    "    return_dict = run_until_resolved(sub_transcripts, questions_list, return_dict, MODEL_TO_EVALUATE)\n",
    "    \n",
    "    n_subs = len(sub_transcripts)\n",
    "    n_missing = 0\n",
    "\n",
    "    for (subject_i, subject) in enumerate(sub_transcripts):\n",
    "        transcript = '\\n'.join([x.replace('\\n', '') for x in sub_transcripts[subject]])\n",
    "        for question_i in range(len(questions_list)):\n",
    "            name = f'{subject}-{question_i}'\n",
    "            if name not in return_dict:\n",
    "                n_missing += 1\n",
    "                print(name)\n",
    "\n",
    "    assert n_missing == 0\n",
    "    \n",
    "    out_dict = dict()\n",
    "    sub_ids = []\n",
    "    for subject in sub_transcripts:\n",
    "        for question_i in range(1, len(questions_list)+1):\n",
    "            if subject not in out_dict:\n",
    "                out_dict[subject] = {}\n",
    "            v = return_dict[f'{subject}-{question_i-1}'][0]\n",
    "            if v is None: raise Exception(f'{subject}-{question_i} is None')\n",
    "            out_dict[subject][f'question_{question_i}']  = v\n",
    "        sub_ids.append(subject)\n",
    "        \n",
    "    out_df = pd.DataFrame(out_dict).T\n",
    "    out_df = out_df[[f'question_{i+1}' for i in range(len(questions_list))]]\n",
    "    out_df['subject'] = pd.DataFrame(out_dict).T.index\n",
    "    out_df['participantID'] = sub_ids\n",
    "\n",
    "    name_mapping_aapecs = {\n",
    "        'O': 'neoOpenness',\n",
    "        'C': 'neoConscientiousness',\n",
    "        'E': 'neoExtraversion',\n",
    "        'A': 'neoAgreeableness',\n",
    "        'N': 'neoNeuroticism'\n",
    "    }\n",
    "\n",
    "    gpt_neo_scores = pd.DataFrame()\n",
    "    gpt_neo_scores['participantID'] = out_df.index.values\n",
    "    for dim in ['O', 'C', 'E', 'A', 'N']:\n",
    "        dim_questions = [x['text'].split(':')[0].lower().replace(' ', '_') for x in questions_list if x['domain']==dim]\n",
    "        is_reverse_coded = [questions_list[q_i-1]['keyed']!='plus' for q_i in [int(x.split('_')[1]) for x in dim_questions]]\n",
    "        v = [np.mean(map_responses_to_integers(a, is_reverse_coded)) for a in out_df[dim_questions].values]\n",
    "        gpt_neo_scores[name_mapping_aapecs[dim]] = v\n",
    "        \n",
    "    scales_df = pd.read_csv(f'{DATA_ROOT}/selfReport.csv')\n",
    "    scales_df = scales_df[scales_df.participantID.isin([int(x) for x in sub_lengths])]\n",
    "    scales_df = scales_df[['participantID']+list(cols_of_interest)]\n",
    "\n",
    "    model_canonical_name = MODEL_TO_EVALUATE[-1]\n",
    "    gpt_neo_scores.to_csv(f'{DATA_ROOT}/aapecs_{model_canonical_name}_text_per_question_scores.csv')\n",
    "    \n",
    "    out_df = pd.DataFrame(out_dict).T\n",
    "    out_df = out_df[[f'question_{i+1}' for i in range(len(questions_list))]]\n",
    "    out_df['subject'] = pd.DataFrame(out_dict).T.index\n",
    "    out_df['participantID'] = out_df.index.values\n",
    "    out_df.to_csv(f'{DATA_ROOT}/aapecs_{model_canonical_name}_text_per_question_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174aeb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59582440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6950da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97bf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db8e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_testing",
   "language": "python",
   "name": "llm_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
