{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0f525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:56.462784Z",
     "start_time": "2025-06-15T23:19:56.450442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "def set_css_in_cell_output():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
    "            .widget-label {color: #d5d5d5 !important;}\n",
    "        </style>\n",
    "    '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css_in_cell_output)\n",
    "from IPython.core import ultratb\n",
    "ultratb.VerboseTB._tb_highlight = \"bg:#0D0D0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec3d23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:57.781156Z",
     "start_time": "2025-06-15T23:19:56.646404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tiktoken\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from jsonschema import validate\n",
    "from openai import OpenAI, RateLimitError, APITimeoutError, InternalServerError, Timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_incrementing, retry_if_exception_type, after_log, before_sleep_log\n",
    "import logging\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.136960Z",
     "start_time": "2025-06-15T23:19:57.784118Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = ''        # private openrouter api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c39d76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.142300Z",
     "start_time": "2025-06-15T23:19:58.138324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "request_limit_per_minute = 500\n",
    "token_limit_per_minute = 2e6\n",
    "\n",
    "request_timeout_seconds = 120   # maximum wait time for openAI to respond before triggering request timeout \n",
    "request_max_retries = 1         # number to times to automatically retry failed requests\n",
    "tpm_wait_polling_seconds = 10    # if our internal TPM estimate thinks TPM limit is exceeded, how often to check if limit cleared\n",
    "\n",
    "# global logger for static classes\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbfee26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.191546Z",
     "start_time": "2025-06-15T23:19:58.167958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ChatGPT:\n",
    "    def __init__(self, model_provider_order,\n",
    "                 halt_on_error=True,\n",
    "                 is_verbose=True,\n",
    "                 timeout=request_timeout_seconds,\n",
    "                 max_retries=request_max_retries,\n",
    "                 request_limit_per_minute=request_limit_per_minute,\n",
    "                 token_limit_per_minute=token_limit_per_minute,\n",
    "                 tpm_wait_polling_seconds=tpm_wait_polling_seconds,\n",
    "                 logger=logger,\n",
    "                 api_key=OPENROUTER_API_KEY,\n",
    "                 limit_manager_db_password=LIMIT_MANAGER_DB_PASSWORD):\n",
    "        self.model, self.provider_order, self.model_canonical_name = model_provider_order\n",
    "        self.halt_on_error = halt_on_error\n",
    "        self.is_verbose = is_verbose\n",
    "        self.tpm_wait_polling_seconds = tpm_wait_polling_seconds\n",
    "        self.request_limit_per_minute = request_limit_per_minute\n",
    "        self.request_delay_seconds = 60.0 / request_limit_per_minute\n",
    "        self.token_limit_per_minute = token_limit_per_minute\n",
    "        self.response_history = []\n",
    "        self.message_history = {}\n",
    "        self.logger = logger\n",
    "        self.limit_manager_db_password = limit_manager_db_password\n",
    "        likert_options = [\n",
    "            \"Strongly Disagree\",\n",
    "            \"Strongly Agree\",\n",
    "            \"Disagree\",\n",
    "            \"Neutral\",\n",
    "            \"Agree\",\n",
    "        ]\n",
    "        # Sort by length to match longer options (e.g., \"Strongly Agree\") before shorter ones (e.g., \"Agree\")\n",
    "        self.likert_options = sorted(likert_options, key=len, reverse=True)\n",
    "        self.default_seed = 1 if 'x-ai' in self.model else 0\n",
    "        \n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\",\n",
    "                             api_key = api_key,\n",
    "                             timeout=timeout,\n",
    "                             max_retries=max_retries)\n",
    "\n",
    "        \n",
    "    def extract_likert_response(self, content):\n",
    "        content_lower = content.lower()\n",
    "        for option in self.likert_options:\n",
    "            pattern = r'\\b' + re.escape(option.lower()) + r'\\b'\n",
    "            match = re.search(pattern, content_lower)\n",
    "            if match:\n",
    "                return json.dumps({\"response\": option})\n",
    "        raise Exception(\"No Likert match found in: \", content)    \n",
    "    \n",
    "    \n",
    "    def get_running_cost_num_prompt_completion_tokens(self):\n",
    "        \"\"\"\n",
    "        This function computes the total cost (estimated) of all\n",
    "        messages sent by the instance of ChatGPT called from\n",
    "        Returns: total_running_cost, total_num_prompt_tokens, total_num_response_tokens\n",
    "        \"\"\"\n",
    "        n_prompt_tokens = np.sum([x.usage.prompt_tokens for x in self.response_history])\n",
    "        n_completion_tokens = np.sum([x.usage.completion_tokens for x in self.response_history])\n",
    "        total_cost = sum(r.usage.cost for r in self.response_history)\n",
    "        return (total_cost,\n",
    "                n_prompt_tokens,\n",
    "                n_completion_tokens)\n",
    "\n",
    "    def get_key_usage_credits(self):\n",
    "        # get what OpenRouter says the api key has used in total\n",
    "        # returns usage, total credits available\n",
    "        resp = requests.get(\n",
    "            \"https://openrouter.ai/api/v1/credits\",\n",
    "            headers={\"Authorization\": f\"Bearer {self.client.api_key}\"}\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        info = resp.json()[\"data\"]\n",
    "        return info[\"total_usage\"], info[\"total_credits\"]\n",
    "\n",
    "    # retry failing requests starting with 10 second wait,\n",
    "    # increasing wait time by 10 seconds each retry, up to a max window of 120s (or 5 times)\n",
    "    # the goal is to try to avoid hitting backoff,\n",
    "    # we treat this as a last resort because of its runtime cost\n",
    "    @retry(wait=wait_incrementing(start=10, increment=10, max=120),\n",
    "           stop=stop_after_attempt(5),\n",
    "           retry=retry_if_exception_type((RateLimitError, APITimeoutError, InternalServerError, Timeout)),\n",
    "           before_sleep=before_sleep_log(logger, logging.INFO),\n",
    "           after=after_log(logger, logging.INFO))\n",
    "    def completion_with_backoff(self, client, **kwargs):\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "\n",
    "\n",
    "    def check_internal_TPM_tracker(self, n_message_tokens):\n",
    "        \"\"\"\n",
    "        Checks internal TPM count to see if a message with length = n_message_tokens\n",
    "        can be sent. If not, it waits (sleeps - blocking) until the message delivery\n",
    "        meets into TPM limit\n",
    "        \"\"\"\n",
    "        now = datetime.datetime.now()\n",
    "        one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "        self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "        n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        # fixed delay waiting if TPM exceeded over past minute\n",
    "        # this is cpu polling, so it doesnt cost money or much compute\n",
    "        while n_tokens_past_minute > self.token_limit_per_minute:\n",
    "            if self.is_verbose: self.logger.info(f'Internal TPM limit exceeded, waiting for {self.tpm_wait_polling_seconds} seconds...')\n",
    "            time.sleep(self.tpm_wait_polling_seconds)\n",
    "            now = datetime.datetime.now()\n",
    "            one_minute_ago = now + datetime.timedelta(seconds=-60)\n",
    "            self.token_count_history = [x for x in self.token_count_history if x[1] > one_minute_ago]\n",
    "            n_tokens_past_minute = np.sum([x[0] for x in self.token_count_history]) + n_message_tokens\n",
    "        now = datetime.datetime.now()\n",
    "        self.token_count_history.append((n_message_tokens, now))\n",
    "\n",
    "\n",
    "    def send_message(self, system_role, message, json_schema, validate_response=True):\n",
    "        \"\"\"\n",
    "        This is the primary function used to send messages to GPT and get responses\n",
    "        Steps are:\n",
    "          - check that json schema meets our basic requirements\n",
    "          - handle RPM and TPM limits as best as we can\n",
    "            (when openai rejects requests for exceeding limits its much slower)\n",
    "          - build and send the message using openai ChatCompletion api\n",
    "          - perform basic validation on GPT's response\n",
    "        This function either returns a ChatCompletion response object or None (if failure occurred)\n",
    "        Errors are propogated using raised Exceptions\n",
    "        \"\"\"\n",
    "        # sleep based on RPM limit (lazy logic, avoids keeping running count of actual requests per minute)\n",
    "        time.sleep(self.request_delay_seconds)\n",
    "\n",
    "        # check TPM limit (not lazy, uses running count of tokens per minute)\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "        n_message_tokens = len(encoding.encode(system_role)) + len(encoding.encode(message))\n",
    "        self.logger.info(f'processing message with {n_message_tokens} tokens...')\n",
    "        if n_message_tokens > self.token_limit_per_minute:\n",
    "            return self.bad_response_output(f'Unable to send message as it exceeds TPM. Number of tokens in message = {n_message_tokens}')\n",
    "                \n",
    "        # build and send message over openai-api\n",
    "        message_id = len(self.message_history.keys())\n",
    "        self.message_history[message_id] = [] if 'x-ai' in self.model else [{\"role\": \"system\", \"content\": system_role}]\n",
    "        self.message_history[message_id].append({\"role\": \"user\", \"content\": message})\n",
    "        try:\n",
    "            response = self.completion_with_backoff(self.client,\n",
    "                                                    model=self.model,\n",
    "                                                    messages=self.message_history[message_id],\n",
    "                                                    temperature=0,\n",
    "                                                    stream=False,\n",
    "                                                    extra_body={\"usage\": {\"include\": True},\n",
    "                                                                \"reasoning\": {# One of the following (not both):\n",
    "                                                                              \"effort\": \"medium\", # Can be \"high\", \"medium\", or \"low\" (OpenAI-style)\n",
    "                                                                              # Optional: Default is false. All models support this.\n",
    "                                                                              \"exclude\": False # Set to true to exclude reasoning tokens from response\n",
    "                                                                              },\n",
    "                                                                \"provider\": {\"order\": self.provider_order, \n",
    "                                                                             \"sort\": \"price\",\n",
    "                                                                             \"data_collection\": \"deny\",\n",
    "                                                                             \"allow_fallbacks\": False}},\n",
    "                                                    response_format={\"type\": \"json_schema\",\n",
    "                                                                     \"json_schema\": json_schema},\n",
    "\n",
    "                                                    seed=self.default_seed, logprobs=False)\n",
    "            \n",
    "            self.response_history.append(response)\n",
    "            # reasoning models dont return a content field\n",
    "            if response.choices[0].message.content is None:\n",
    "                self.bad_response_output(f'None in message content')\n",
    "                return None\n",
    "            elif response.choices[0].message.content == '':\n",
    "                if hasattr(response.choices[0].message, 'reasoning'):\n",
    "                    if response.choices[0].message.reasoning != '':\n",
    "                        response.choices[0].message.content = response.choices[0].message.reasoning\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            if self.halt_on_error:\n",
    "                raise\n",
    "            else:\n",
    "                if self.is_verbose:\n",
    "                    str_e = str(e)\n",
    "                    self.logger.info(f'An exception occurred: {str_e}')\n",
    "                    self.logger.info(traceback.format_exc())\n",
    "                return None\n",
    "\n",
    "        return (response, message_id)\n",
    "        \n",
    "\n",
    "    def bad_response_output(self, error):\n",
    "        # general function for informing the user when an error occurs\n",
    "        if self.halt_on_error:\n",
    "            raise Exception(error)\n",
    "        else:\n",
    "            if self.is_verbose:\n",
    "                self.logger.info(f'Error - {error}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c0808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.197562Z",
     "start_time": "2025-06-15T23:19:58.192752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_4_maverick = ('meta-llama/llama-4-maverick', ['Fireworks', 'Together', 'Kluster'], 'llama_maverick')\n",
    "gemini_flash = ('google/gemini-2.5-flash-preview-05-20:thinking', ['Google', 'Google AI Studio'], 'gemini_flash')\n",
    "qwen_235B = ('qwen/qwen3-235b-a22b', ['DeepInfra', 'Kluster', 'Parasail', 'Together', 'Nebius'], 'qwen3_235B')  \n",
    "gpt_41 = ('openai/gpt-4.1', ['OpenAI'], 'gpt_41')\n",
    "gpt_41_mini = ('openai/gpt-4.1-mini', ['OpenAI'], 'gpt_41_mini')\n",
    "claude_sonnet = ('anthropic/claude-3.7-sonnet', ['Anthropic', 'Amazon Bedrock', 'Google', 'Google AI Studio'], 'claude_sonnet')\n",
    "grok_3 = ('x-ai/grok-3-beta', ['xAI'], 'grok3_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ac189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5aa97a4",
   "metadata": {},
   "source": [
    "# Example: We expect a simple string response from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_EVALUATE = gpt_41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0cf7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.463305Z",
     "start_time": "2025-06-15T23:19:58.459387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify GPT output json schema for a simple string response\n",
    "# all response schemas must contain \"refusal\" and \"reason_for_refusal\" fields\n",
    "simple_string_response_json = {\n",
    "    \"name\": \"simple_string_response_json\",\n",
    "    \"description\": \"Schema for a simple string response with refusal tracking\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"description\": \"JSON schema for a simple string response\",\n",
    "        \"properties\": {\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The generated output by GPT, formatted as a plain string\"\n",
    "            }\n",
    "        },\n",
    "        \"additionalProperties\": False,\n",
    "        \"required\": [\"response\"]\n",
    "    },\n",
    "    \"strict\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95702c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:19:58.856836Z",
     "start_time": "2025-06-15T23:19:58.850961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def example_messaging_wrapper(chat, system_role, message, json_schema):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    response, message_history_id = None, -1\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message, json_schema=json_schema,\n",
    "                                                         validate_response=True)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        response_str = response_json[\"response\"]\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        response_str = ''\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    last_messages_sent_to_gpt = '' if (message_history_id not in chat.message_history) else chat.message_history[message_history_id]\n",
    "    print(f'Messages to GPT:\\n{last_messages_sent_to_gpt}')\n",
    "    print(f'Response from GPT:\\n{response_str}')\n",
    "    print(f'Cost: ${cost:.5f}')\n",
    "    \n",
    "    return response, message_history_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aeeb7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:02.625449Z",
     "start_time": "2025-06-15T23:19:59.495274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 13 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages to GPT:\n",
      "[{'role': 'system', 'content': 'you are a helpful assistant.'}, {'role': 'user', 'content': 'help me bake a vanilla cake.'}]\n",
      "Response from GPT:\n",
      "Sure! Here’s a simple recipe to bake a vanilla cake:\n",
      "\n",
      "**Ingredients:**\n",
      "- 1 and 1/2 cups (190g) all-purpose flour\n",
      "- 1 cup (200g) sugar\n",
      "- 1/2 cup (115g) unsalted butter, softened\n",
      "- 2 large eggs\n",
      "- 1/2 cup (120ml) milk\n",
      "- 2 teaspoons vanilla extract\n",
      "- 1 and 3/4 teaspoons baking powder\n",
      "- 1/4 teaspoon salt\n",
      "\n",
      "**Instructions:**\n",
      "1. Preheat your oven to 350°F (175°C). Grease and flour an 8-inch round cake pan.\n",
      "2. In a bowl, whisk together the flour, baking powder, and salt.\n",
      "3. In another bowl, beat the butter and sugar together until light and fluffy.\n",
      "4. Add the eggs one at a time, beating well after each addition. Stir in the vanilla extract.\n",
      "5. Add the dry ingredients to the wet mixture in three parts, alternating with the milk. Begin and end with the dry ingredients. Mix until just combined.\n",
      "6. Pour the batter into the prepared pan and smooth the top.\n",
      "7. Bake for 30-35 minutes, or until a toothpick inserted into the center comes out clean.\n",
      "8. Let the cake cool in the pan for 10 minutes, then turn out onto a wire rack to cool completely.\n",
      "\n",
      "Enjoy your homemade vanilla cake! Let me know if you need help with frosting or decoration ideas.\n",
      "Cost: $0.00275\n"
     ]
    }
   ],
   "source": [
    "# specify system role and user message\n",
    "system_role = 'you are a helpful assistant.'\n",
    "message = f'help me bake a vanilla cake.'\n",
    "\n",
    "# create a single instance of ChatGPT \n",
    "# so that we can keep track of running costs\n",
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "response, message_history_id = example_messaging_wrapper(chat, system_role, message, simple_string_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09d979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b01098",
   "metadata": {},
   "source": [
    "# SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec27c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:03.620929Z",
     "start_time": "2025-06-15T23:20:02.627890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "509eb846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:03.631581Z",
     "start_time": "2025-06-15T23:20:03.622502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = './data/SST/'  # please contact the authors for access to the data\n",
    "\n",
    "good_th_dict = pickle.load(open(f'{DATA_ROOT}/SST_data.pickle', 'rb'))\n",
    "print(len([x for x in good_th_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c24367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:03.639666Z",
     "start_time": "2025-06-15T23:20:03.633408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"I am not a worrier.\",\n",
    "    \"I like to have a lot of people around me.\",\n",
    "    \"I don’t like to waste my time daydreaming.\",\n",
    "    \"I try to be courteous to everyone I meet.\",\n",
    "    \"I keep my belongings neat and clean.\",\n",
    "    \"I often feel inferior to others.\",\n",
    "    \"I laugh easily.\",\n",
    "    \"Once I find the right way to do something I stick to it.\",\n",
    "    \"I often get into arguments with my family and co-workers.\",\n",
    "    \"I’m pretty good about pacing myself so as to get things done on time.\",\n",
    "    \"When I’m under a great deal of stress sometimes I feel like I’m going to pieces.\",\n",
    "    \"I don’t consider myself especially 'light-hearted.'\",\n",
    "    \"I am intrigued by the patterns I find in art and nature.\",\n",
    "    \"Some people think I’m selfish and egotistical.\",\n",
    "    \"I am not a very methodical person.\",\n",
    "    \"I rarely feel lonely or blue.\",\n",
    "    \"I really enjoy talking to people.\",\n",
    "    \"I believe letting students hear controversial speakers can only confuse and mislead them.\",\n",
    "    \"I would rather cooperate with others than compete with them.\",\n",
    "    \"I try to perform all the tasks assigned to me conscientiously.\",\n",
    "    \"I often feel tense and jittery.\",\n",
    "    \"I like to be where the action is.\",\n",
    "    \"Poetry has little or no effect on me.\",\n",
    "    \"I tend to be cynical and skeptical of others’ intentions.\",\n",
    "    \"I have a clear set of goals and work toward them in an orderly fashion.\",\n",
    "    \"Sometimes I feel completely worthless.\",\n",
    "    \"I usually prefer to do things alone.\",\n",
    "    \"I often try new and foreign foods.\",\n",
    "    \"I believe that most people will take advantage of you if you let them.\",\n",
    "    \"I waste a lot of time before settling down to work.\",\n",
    "    \"I rarely feel fearful or anxious.\",\n",
    "    \"I often feel as if I’m bursting with energy.\",\n",
    "    \"I seldom notice the moods or feelings that different environments produce.\",\n",
    "    \"Most people I know like me.\",\n",
    "    \"I work hard to accomplish my goals.\",\n",
    "    \"I often get angry at the way people treat me.\",\n",
    "    \"I am a cheerful high-spirited person.\",\n",
    "    \"I believe we should look to our religious authorities for decisions on moral issues.\",\n",
    "    \"Some people think of me as cold and calculating.\",\n",
    "    \"When I make a commitment I can always be counted on to follow through.\",\n",
    "    \"Too often when things go wrong I get discouraged and feel like giving up.\",\n",
    "    \"I am not a cheerful optimist.\",\n",
    "    \"Sometimes when I am reading poetry or looking at a work of art I feel a chill or wave of excitement.\",\n",
    "    \"I’m hard-headed and tough-minded in my attitudes.\",\n",
    "    \"Sometimes I’m not as dependable or reliable as I should be.\",\n",
    "    \"I am seldom sad or depressed.\",\n",
    "    \"My life is fast-paced.\",\n",
    "    \"I have little interest in speculating on the nature of the universe or the human condition.\",\n",
    "    \"I generally try to be thoughtful and considerate.\",\n",
    "    \"I am a productive person who always gets the job done.\",\n",
    "    \"I often feel helpless and want someone else to solve my problems.\",\n",
    "    \"I am a very active person.\",\n",
    "    \"I have a lot of intellectual curiosity.\",\n",
    "    \"If I don’t like people I let them know it.\",\n",
    "    \"I never seem to be able to get organized.\",\n",
    "    \"At times I have been so ashamed I just wanted to hide.\",\n",
    "    \"I would rather go my own way than be a leader of others.\",\n",
    "    \"I often enjoy playing with theories or abstract ideas.\",\n",
    "    \"If necessary I am willing to manipulate people to get what I want.\",\n",
    "    \"I strive for excellence in everything I do.\"\n",
    "]\n",
    "for i in range(len(questions)):\n",
    "    questions[i] = f'Question {i+1}: ' + questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "399ca6e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:03.649309Z",
     "start_time": "2025-06-15T23:20:03.645498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Question 1: I am not a worrier.',\n",
       " 'Question 2: I like to have a lot of people around me.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ab56af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:06.782271Z",
     "start_time": "2025-06-15T23:20:06.777606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_NEO_FFI_prompt(thoughts, questions):\n",
    "    prompt_template = f\"\"\"\n",
    "Your task is to respond to the following NEO-FFI questions based on the participant's spontaneous stream of thoughts, provided below. Respond as though you are the individual who generated these thoughts, reflecting their personality traits.\n",
    "Base your answer on inferred personality traits. Think carefully about what the thoughts imply about tendencies and behaviors.\n",
    "\n",
    "NEO-FFI questions to answer:\n",
    "{questions}\n",
    "\n",
    "For each question, select the most appropriate option:\n",
    "- Strongly Disagree: The statement is definitely false or the participant would strongly disagree with it.\n",
    "- Disagree: The statement is mostly false or the participant would generally disagree with it.\n",
    "- Neutral: The participant would be neutral on the statement, cannot decide, or find the statement equally true and false.\n",
    "- Agree: The statement is mostly true or the participant would generally agree with it.\n",
    "- Strongly Agree: The statement is definitely true or the participant would strongly agree with it.\n",
    "\n",
    "Then:\n",
    "Provide 3-5 high-level themes that explain *why* you gave the ratings above. Do not provide one theme per question, instead focus on the most significant patterns or insights that emerge across the questions above. \n",
    "For each theme, include:\n",
    "  - A brief explanation of a theme that informed your judgment.\n",
    "  - All direct quotes from the participant's stream of thoughts that support the theme and explanation.\n",
    "  - Remember: Do not paraphrase or invent quotes, the quotes must be exactly as given in the participant's stream of thoughts below.\n",
    "\n",
    "Participant's spontaneous stream of thoughts:\n",
    "{thoughts}\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d094e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:06.903920Z",
     "start_time": "2025-06-15T23:20:06.899256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimension_questions = {\n",
    "    'Neuroticism': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56],\n",
    "    'Extraversion': [2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 57],\n",
    "    'Openness': [3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 53, 58],\n",
    "    'Agreeableness': [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59],\n",
    "    'Conscientiousness': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4126c8b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:07.498647Z",
     "start_time": "2025-06-15T23:20:07.492317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_required_fields(schema, parent_key=''):\n",
    "    required_fields = []\n",
    "    if 'required' in schema:\n",
    "        # If parent_key exists, prefix it to the required field names\n",
    "        for field in schema['required']:\n",
    "            full_field_name = f\"{parent_key}.{field}\" if parent_key else field\n",
    "            required_fields.append(full_field_name)\n",
    "\n",
    "    if 'properties' in schema:\n",
    "        for key, value in schema['properties'].items():\n",
    "            new_parent_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "            required_fields.extend(find_required_fields(value, new_parent_key))\n",
    "\n",
    "    return required_fields\n",
    "\n",
    "\n",
    "def get_value_from_path(data, path):\n",
    "    keys = path.split('.')\n",
    "    for key in keys:\n",
    "        if isinstance(data, list):\n",
    "            key = int(key)\n",
    "        data = data[key]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_required_values(schema, response):\n",
    "    required_fields = find_required_fields(schema)\n",
    "    required_values = {}\n",
    "\n",
    "    for field in required_fields:\n",
    "        try:\n",
    "            value = get_value_from_path(response, field)\n",
    "            required_values[field] = value\n",
    "        except KeyError:\n",
    "            required_values[field] = None  # Handle missing values if needed\n",
    "\n",
    "    return required_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b7db536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:07.941108Z",
     "start_time": "2025-06-15T23:20:07.936157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generic_messaging_wrapper(chat, system_role, message, json_schema):\n",
    "    # with halt_on_error set to True in ChatGPT class, \n",
    "    # we use exception propogation to handle errors and edge-cases\n",
    "    message_history_id = -1\n",
    "    required_values = None\n",
    "    try:\n",
    "        response, message_history_id = chat.send_message(system_role=system_role,\n",
    "                                                         message=message, json_schema=json_schema)\n",
    "        assert response is not None\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        required_values = response_json\n",
    "    except Exception as e:\n",
    "        response_json = {}\n",
    "        chat.logger.info(f'Messaging wrapper failure - {str(e)}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    cost, n_prompt_tokens, n_completion_tokens = chat.get_running_cost_num_prompt_completion_tokens()\n",
    "    return required_values, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3972fa8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:08.304809Z",
     "start_time": "2025-06-15T23:20:08.301212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_role = ''\n",
    "print(system_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b6367b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:08.634447Z",
     "start_time": "2025-06-15T23:20:08.626484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def generate_neo_schema(questions):\n",
    "    \"\"\"\n",
    "    Given a list of strings like \"Question 1: I am not a worrier.\",\n",
    "    returns a JSON‐schema dict where each question becomes an enum‐string field\n",
    "    (Strongly Disagree … Strongly Agree), plus a 'justifications' array.\n",
    "    \"\"\"\n",
    "    OPTIONS = [\n",
    "        \"Strongly Disagree\",\n",
    "        \"Disagree\",\n",
    "        \"Neutral\",\n",
    "        \"Agree\",\n",
    "        \"Strongly Agree\"\n",
    "    ]\n",
    "\n",
    "    properties = {}\n",
    "    required = []\n",
    "    \n",
    "    for q in questions:\n",
    "        # e.g. \"Question 1\" → \"Question_1\"\n",
    "        key = q.split(':')[0].replace(' ', '_')\n",
    "        properties[key] = {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": f\"Response to '{q.strip()}'\",\n",
    "            \"enum\": OPTIONS\n",
    "        }\n",
    "        required.append(key)\n",
    "    \n",
    "    # justifications stays the same\n",
    "    properties[\"justifications\"] = {\n",
    "        \"type\": \"array\",\n",
    "        \"description\": \"Each entry provides an explanation and supporting quotes.\",\n",
    "        \"items\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"explanation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A brief explanation of a theme or observation.\"\n",
    "                },\n",
    "                \"quotes\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"Direct quotes from the stream of thoughts that support the explanation.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"additionalProperties\": False,\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The exact quote.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"explanation\", \"quotes\"]\n",
    "        }\n",
    "    }\n",
    "    required.append(\"justifications\")\n",
    "\n",
    "    schema = {\n",
    "        \"name\": \"neo_ffi_assessment_from_stream_of_thoughts\",\n",
    "        \"description\": (\n",
    "            \"Rates each NEO-FFI item from participant’s spontaneous stream of thoughts, \"\n",
    "            \"plus structured justifications with supporting quotes.\"\n",
    "        ),\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"strict\": True,\n",
    "            \"properties\": properties,\n",
    "            \"required\": required\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05b65fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:09.224262Z",
     "start_time": "2025-06-15T23:20:09.218612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import difflib\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(s, case_insensitive=True, unicode_normalize=True):\n",
    "    \"\"\"\n",
    "    - Strip leading/trailing whitespace\n",
    "    - Collapse all internal whitespace to single spaces\n",
    "    - Optionally lowercase\n",
    "    - Optionally apply Unicode NFC normalization\n",
    "    \"\"\"\n",
    "    # Unicode normalization (e.g. é → e + ´)\n",
    "    if unicode_normalize:\n",
    "        s = unicodedata.normalize('NFC', s)\n",
    "    # Collapse whitespace\n",
    "    s = ' '.join(s.split())\n",
    "    # Lowercase if desired\n",
    "    if case_insensitive:\n",
    "        s = s.lower()\n",
    "    s = s.replace('\\n', ' ')\n",
    "    return s\n",
    "\n",
    "def longest_common_substring(a_raw, b_raw):\n",
    "    \"\"\"\n",
    "    Returns the longest substring common to both a and b.\n",
    "    Uses difflib.SequenceMatcher under the hood.\n",
    "    \"\"\"\n",
    "    a = normalize_text(a_raw)\n",
    "    b = normalize_text(b_raw)\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(None, a, b)\n",
    "    match = matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "    if match.size == 0: return ''\n",
    "    return a[match.a : match.a + match.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12f2f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:09.881990Z",
     "start_time": "2025-06-15T23:20:09.875176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_neo_summary(data, thoughts):\n",
    "    \"\"\"\n",
    "    Given a dict matching your NEO‐FFI schema—\n",
    "    with keys like \"Question_1\", \"Question_6\", … and a \"justifications\" list—\n",
    "    returns a nicely formatted multi‐line string.\n",
    "    \"\"\"\n",
    "    # 1) Collect and sort the question keys by their numeric index\n",
    "    q_keys = [k for k in data.keys() if re.match(r\"Question_\\d+$\", k)]\n",
    "    q_keys.sort(key=lambda k: int(k.split(\"_\")[1]))\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    \"\"\"\n",
    "    # 2) Add each question + response\n",
    "    for key in q_keys:\n",
    "        # turn \"Question_1\" → \"Question 1\"\n",
    "        pretty = key.replace(\"_\", \" \")\n",
    "        resp = data[key]\n",
    "        lines.append(f\"{pretty}: {resp}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # 3) Add a spacer before justifications\n",
    "    lines.append(\"Justifications:\\n\")\n",
    "    \n",
    "    # 4) Enumerate through each justification entry\n",
    "    for i, entry in enumerate(data.get(\"justifications\", []), start=1):\n",
    "        lines.append(f\"Reason {i}\")\n",
    "        lines.append(entry[\"explanation\"])\n",
    "        \n",
    "        # Citation header\n",
    "        n_quotes = len(entry[\"quotes\"])\n",
    "        if   n_quotes == 1: lines.append(\"  Citation:\")\n",
    "        elif n_quotes > 1:  lines.append(\"  Citations:\")\n",
    "        \n",
    "        # The quotes themselves\n",
    "        for quote in entry[\"quotes\"]:\n",
    "            matched_quote = longest_common_substring(thoughts, quote['text'])\n",
    "            if len(matched_quote) > 0:\n",
    "                if len(matched_quote.split(' ')) > 4:\n",
    "                    lines.append(f\"    \\\"{matched_quote.strip()}\\\"\")\n",
    "        \n",
    "        # blank line between reasons\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92b81855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:10.545295Z",
     "start_time": "2025-06-15T23:20:10.537437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_neo_trait(response, trait):\n",
    "    # Scoring mapping\n",
    "    scoring_map = {'Strongly Disagree': 0, 'Disagree': 1, 'Neutral': 2, 'Agree': 3, 'Strongly Agree': 4}\n",
    "\n",
    "    # Negative scoring mapping (reverse scoring)\n",
    "    reverse_scoring_map = {'Strongly Disagree': 4, 'Disagree': 3, 'Neutral': 2, 'Agree': 1, 'Strongly Agree': 0}\n",
    "\n",
    "    # Define the items for each dimension based on the provided scoring guide\n",
    "    dimensions = {\n",
    "        'Neuroticism': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56],\n",
    "        'Extraversion': [2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 57],\n",
    "        'Openness': [3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 53, 58],\n",
    "        'Agreeableness': [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59],\n",
    "        'Conscientiousness': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "    }\n",
    "\n",
    "    # Reverse scoring items based on the provided scoring guide\n",
    "    reverse_items = [1, 16, 31, 46, 12, 27, 42, 57, 3, 8, 18, 23, 38, 48, 9, 14, 24, 29, 39, 44, 54, 59, 15, 30, 45, 55]\n",
    "\n",
    "    total = 0\n",
    "    for item in dimensions[trait]:\n",
    "        key = f\"Question_{item}\"\n",
    "        if key not in response:\n",
    "            raise KeyError(f\"Missing response for {key}\")\n",
    "        answer = response[key]\n",
    "        if answer not in scoring_map:\n",
    "            raise ValueError(f\"Unrecognized response {answer!r} for {key}\")\n",
    "\n",
    "        # choose the correct map\n",
    "        if item in reverse_items:\n",
    "            total += reverse_scoring_map[answer]\n",
    "        else:\n",
    "            total += scoring_map[answer]\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c52caa2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:20.744367Z",
     "start_time": "2025-06-15T23:20:11.184337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 4680 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Justifications:\n",
      "\n",
      "Reason 1\n",
      "Generally positive mood and low sadness or depression. The participant expresses enjoyment in daily activities, looks forward to things, and does not dwell on negative emotions.\n",
      "  Citations:\n",
      "    \"i like long bus rides i feel its very calming\"\n",
      "    \"i just i just want to stay in bed and watch tv\"\n",
      "    \"im excited for the christmas food because then we can eat a lot and have presents\"\n",
      "    \"i think my brain is just all calm right now\"\n",
      "    \"i guess i dont really have any private thoughts\"\n",
      "\n",
      "Reason 2\n",
      "Low self-consciousness and low feelings of inferiority or worthlessness. The participant does not express shame, self-hate, or a sense of being less than others.\n",
      "  Citations:\n",
      "    \"i feel like i need to wash my face every day i only wash my face like sometimes like every other day but i need to wash it every single day so i wont be dry\"\n",
      "    \"i think i need to when i get home i need to eat\"\n",
      "    \"i think my brain is just all calm right now\"\n",
      "\n",
      "Reason 3\n",
      "Mild worry and stress, but not excessive. The participant mentions being busy and having a lot to do, but does not catastrophize or express feeling overwhelmed or panicked.\n",
      "  Citations:\n",
      "    \"i need to get a good score on this quiz just because i want to and because i like it\"\n",
      "\n",
      "Reason 4\n",
      "Strong social orientation and dislike of loneliness. The participant repeatedly expresses a desire to be with others and discomfort with being alone.\n",
      "  Citations:\n",
      "    \"i miss my family i wanna see my brother\"\n",
      "    \"i dont like eating alone because it makes me feel i dont know it makes me feel weird whenever i step into the dining hall alone i always have to look for someone i know\"\n",
      "    \"i like talking to people and sitting with others so ill probably text someone when i get back to eat with me\"\n",
      "\n",
      "Reason 5\n",
      "Practical approach to problems and some self-efficacy. The participant seeks help when needed but does not express helplessness or a desire for others to solve their problems.\n",
      "  Citation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "\n",
    "trait = 'Neuroticism'\n",
    "subject = '5'\n",
    "\n",
    "thoughts = '\\n'.join([a for b in good_th_dict[subject] for a in b])\n",
    "neu_questions = [questions[x-1] for x in dimension_questions[trait]]\n",
    "neu_schema = generate_neo_schema(neu_questions)\n",
    "\n",
    "message = get_NEO_FFI_prompt(thoughts, '\\n'.join(neu_questions))\n",
    "required_values, cost = generic_messaging_wrapper(chat, system_role, message, neu_schema)\n",
    "\n",
    "trait_score = score_neo_trait(required_values, trait)\n",
    "trait_reasoning = format_neo_summary(required_values, thoughts)\n",
    "\n",
    "print(trait_score)\n",
    "print(trait_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cca8e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:20:31.054396Z",
     "start_time": "2025-06-15T23:20:31.039767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>openness</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  subject  neuroticism  extraversion  openness  agreeableness  \\\n",
       "0           5        5           18            24        23             36   \n",
       "1          17       17           36            29        29             36   \n",
       "2          23       23           32            17        29             20   \n",
       "3          24       24           19            19        24             35   \n",
       "4          29       29           23            23        28             36   \n",
       "\n",
       "   conscientiousness  \n",
       "0                 13  \n",
       "1                 11  \n",
       "2                 12  \n",
       "3                 12  \n",
       "4                 13  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_scores = pd.read_csv(f'{DATA_ROOT}/sst_gpt_41_text_per_question_scores.csv')\n",
    "gpt_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b47da80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:26:39.135299Z",
     "start_time": "2025-06-15T23:20:31.466786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030ad9cba17e4ac092729a59cf561b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 3934 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5197 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 2785 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 6086 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3554 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 4531 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 4635 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3842 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 2202 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 4017 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5196 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 1618 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 2524 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5974 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3750 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 4023 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5247 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5581 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 4269 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3154 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5225 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3065 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 1849 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3399 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:processing message with 2036 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5565 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5477 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 2508 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 3268 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:processing message with 5072 tokens...\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for trait in tqdm(dimension_questions):\n",
    "    trait_scores_df = gpt_scores.sort_values(by=trait.lower(), ascending=True)[['subject', trait.lower()]]\n",
    "    for i in tqdm(range(3), desc=trait, leave=False):\n",
    "        subject, true_trait_score = trait_scores_df.values[i, :]\n",
    "        chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "        thoughts = '\\n'.join([a for b in good_th_dict[str(subject)] for a in b])\n",
    "        neu_questions = [questions[x-1] for x in dimension_questions[trait]]\n",
    "        neu_schema = generate_neo_schema(neu_questions)\n",
    "        message = get_NEO_FFI_prompt(thoughts, neu_questions)\n",
    "        required_values, cost = generic_messaging_wrapper(chat, system_role, message, neu_schema)\n",
    "        gpt_trait_score = score_neo_trait(required_values, trait)\n",
    "        trait_reasoning = format_neo_summary(required_values, thoughts)  \n",
    "        outputs.append((subject, f'low_{trait}', true_trait_score, gpt_trait_score, trait_reasoning))\n",
    "        \n",
    "        subject, true_trait_score = trait_scores_df.values[-(i+1), :]\n",
    "        chat = ChatGPT(model_provider_order=MODEL_TO_EVALUATE)\n",
    "        thoughts = '\\n'.join([a for b in good_th_dict[str(subject)] for a in b])\n",
    "        neu_questions = [questions[x-1] for x in dimension_questions[trait]]\n",
    "        neu_schema = generate_neo_schema(neu_questions)\n",
    "        message = get_NEO_FFI_prompt(thoughts, neu_questions)\n",
    "        required_values, cost = generic_messaging_wrapper(chat, system_role, message, neu_schema)\n",
    "        gpt_trait_score = score_neo_trait(required_values, trait)\n",
    "        trait_reasoning = format_neo_summary(required_values, thoughts)  \n",
    "        outputs.append((subject, f'high_{trait}', true_trait_score, gpt_trait_score, trait_reasoning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da5f7663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:26:39.151569Z",
     "start_time": "2025-06-15T23:26:39.137511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scales_df = pd.read_csv(f'{DATA_ROOT}/SST_scales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1578e136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:26:39.206209Z",
     "start_time": "2025-06-15T23:26:39.152830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .jupyter-widgets {color: #d5d5d5 !important;}\n",
       "            .widget-label {color: #d5d5d5 !important;}\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(outputs)\n",
    "df.columns = ['subject', 'trait_description', 'self_reported', 'gpt_predicted', 'gpt_reasoning']\n",
    "df[['level','trait']] = df['trait_description'].str.split('_', n=1, expand=True)\n",
    "df = df.sort_values(by=['trait', 'level'])\n",
    "\n",
    "true_scales = []\n",
    "for sub_i, sub in enumerate(df.subject.values):\n",
    "    s_trait = df.trait.values[sub_i].lower()\n",
    "    t_v = scales_df[scales_df.subject == sub][s_trait].values.flatten()[0]\n",
    "    true_scales.append(t_v)\n",
    "df['self_reported'] = true_scales\n",
    "\n",
    "df = df[['level', 'trait', 'gpt_reasoning']] #'subject', 'self_reported', 'gpt_predicted']]\n",
    "\n",
    "df.to_csv(f'{DATA_ROOT}/SST_gpt_41_reasoning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28531780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62b16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d7633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3c0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97bf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da19987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_testing",
   "language": "python",
   "name": "llm_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
